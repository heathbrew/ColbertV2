{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a090dd5f-c216-432f-8bcf-60a40650f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "# Optional: Set up debug logging to see what llamaindex is doing\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0e93918-d26d-4db3-84c3-10cca5537139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "Resetting dropped connection: huggingface.co\n",
      "Resetting dropped connection: huggingface.co\n",
      "Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config_sentence_transformers.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config_sentence_transformers.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config_sentence_transformers.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config_sentence_transformers.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/README.md HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/README.md HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/README.md HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/README.md HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/modules.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/sentence_bert_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/sentence_bert_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/sentence_bert_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/sentence_bert_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /BAAI/bge-small-en-v1.5/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5/revision/main HTTP/11\" 200 148798\n",
      "https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5/revision/main HTTP/11\" 200 148798\n",
      "https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5/revision/main HTTP/11\" 200 148798\n",
      "https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5/revision/main HTTP/11\" 200 148798\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5 HTTP/11\" 200 148798\n",
      "https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5 HTTP/11\" 200 148798\n",
      "https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5 HTTP/11\" 200 148798\n",
      "https://huggingface.co:443 \"GET /api/models/BAAI/bge-small-en-v1.5 HTTP/11\" 200 148798\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adcbd8a9-ad54-45d9-beab-ec7e7e71de96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: ColBERTv2:\n",
      "Effective and Efﬁcient Retrieval via...\n",
      "> Adding chunk: ColBERTv2:\n",
      "Effective and Efﬁcient Retrieval via...\n",
      "> Adding chunk: ColBERTv2:\n",
      "Effective and Efﬁcient Retrieval via...\n",
      "> Adding chunk: ColBERTv2:\n",
      "Effective and Efﬁcient Retrieval via...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: a cross-encoder and hard-negative mining (§3.2)...\n",
      "> Adding chunk: a cross-encoder and hard-negative mining (§3.2)...\n",
      "> Adding chunk: a cross-encoder and hard-negative mining (§3.2)...\n",
      "> Adding chunk: a cross-encoder and hard-negative mining (§3.2)...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: COIL (Gao et al., 2021) also generates token-le...\n",
      "> Adding chunk: COIL (Gao et al., 2021) also generates token-le...\n",
      "> Adding chunk: COIL (Gao et al., 2021) also generates token-le...\n",
      "> Adding chunk: COIL (Gao et al., 2021) also generates token-le...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: by PQ via a ranking-oriented loss.\n",
      "SDR (Cohen e...\n",
      "> Adding chunk: by PQ via a ranking-oriented loss.\n",
      "SDR (Cohen e...\n",
      "> Adding chunk: by PQ via a ranking-oriented loss.\n",
      "SDR (Cohen e...\n",
      "> Adding chunk: by PQ via a ranking-oriented loss.\n",
      "SDR (Cohen e...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: In\n",
      "practice, user-facing IR and QA applications...\n",
      "> Adding chunk: In\n",
      "practice, user-facing IR and QA applications...\n",
      "> Adding chunk: In\n",
      "practice, user-facing IR and QA applications...\n",
      "> Adding chunk: In\n",
      "practice, user-facing IR and QA applications...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: vectors are stored. At search time, the query q...\n",
      "> Adding chunk: vectors are stored. At search time, the query q...\n",
      "> Adding chunk: vectors are stored. At search time, the query q...\n",
      "> Adding chunk: vectors are stored. At search time, the query q...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Our results in §5 reveal that such super-\n",
      "visio...\n",
      "> Adding chunk: Our results in §5 reveal that such super-\n",
      "visio...\n",
      "> Adding chunk: Our results in §5 reveal that such super-\n",
      "visio...\n",
      "> Adding chunk: Our results in §5 reveal that such super-\n",
      "visio...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: vector representations. Product quantization (G...\n",
      "> Adding chunk: vector representations. Product quantization (G...\n",
      "> Adding chunk: vector representations. Product quantization (G...\n",
      "> Adding chunk: vector representations. Product quantization (G...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The result\n",
      "passages are then sorted by score an...\n",
      "> Adding chunk: The result\n",
      "passages are then sorted by score an...\n",
      "> Adding chunk: The result\n",
      "passages are then sorted by score an...\n",
      "> Adding chunk: The result\n",
      "passages are then sorted by score an...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Topic\n",
      "Question Set\n",
      "Dev\n",
      "Test\n",
      "# Questions\n",
      "# Passa...\n",
      "> Adding chunk: Topic\n",
      "Question Set\n",
      "Dev\n",
      "Test\n",
      "# Questions\n",
      "# Passa...\n",
      "> Adding chunk: Topic\n",
      "Question Set\n",
      "Dev\n",
      "Test\n",
      "# Questions\n",
      "# Passa...\n",
      "> Adding chunk: Topic\n",
      "Question Set\n",
      "Dev\n",
      "Test\n",
      "# Questions\n",
      "# Passa...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: A: the Fire Nation\n",
      "had wiped out all Airbenders...\n",
      "> Adding chunk: A: the Fire Nation\n",
      "had wiped out all Airbenders...\n",
      "> Adding chunk: A: the Fire Nation\n",
      "had wiped out all Airbenders...\n",
      "> Adding chunk: A: the Fire Nation\n",
      "had wiped out all Airbenders...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Q: what is xerror in rpart?\n",
      "Q: is sub question ...\n",
      "> Adding chunk: Q: what is xerror in rpart?\n",
      "Q: is sub question ...\n",
      "> Adding chunk: Q: what is xerror in rpart?\n",
      "Q: is sub question ...\n",
      "> Adding chunk: Q: what is xerror in rpart?\n",
      "Q: is sub question ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Dev-set results for baseline systems are from t...\n",
      "> Adding chunk: Dev-set results for baseline systems are from t...\n",
      "> Adding chunk: Dev-set results for baseline systems are from t...\n",
      "> Adding chunk: Dev-set results for baseline systems are from t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Corpus\n",
      "Models without Distillation\n",
      "Models with ...\n",
      "> Adding chunk: Corpus\n",
      "Models without Distillation\n",
      "Models with ...\n",
      "> Adding chunk: Corpus\n",
      "Models without Distillation\n",
      "Models with ...\n",
      "> Adding chunk: Corpus\n",
      "Models without Distillation\n",
      "Models with ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Sub-table (a) reports results on BEIR and sub-t...\n",
      "> Adding chunk: Sub-table (a) reports results on BEIR and sub-t...\n",
      "> Adding chunk: Sub-table (a) reports results on BEIR and sub-t...\n",
      "> Adding chunk: Sub-table (a) reports results on BEIR and sub-t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: sages. This is known to lead to artiﬁcial lexic...\n",
      "> Adding chunk: sages. This is known to lead to artiﬁcial lexic...\n",
      "> Adding chunk: sages. This is known to lead to artiﬁcial lexic...\n",
      "> Adding chunk: sages. This is known to lead to artiﬁcial lexic...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: This storage ﬁgure includes 4.5 GiB for storing...\n",
      "> Adding chunk: This storage ﬁgure includes 4.5 GiB for storing...\n",
      "> Adding chunk: This storage ﬁgure includes 4.5 GiB for storing...\n",
      "> Adding chunk: This storage ﬁgure includes 4.5 GiB for storing...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Acknowledgements\n",
      "This research was supported in...\n",
      "> Adding chunk: Acknowledgements\n",
      "This research was supported in...\n",
      "> Adding chunk: Acknowledgements\n",
      "This research was supported in...\n",
      "> Adding chunk: Acknowledgements\n",
      "This research was supported in...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: ColBERTv2\n",
      "performed well in all of these settin...\n",
      "> Adding chunk: ColBERTv2\n",
      "performed well in all of these settin...\n",
      "> Adding chunk: ColBERTv2\n",
      "performed well in all of these settin...\n",
      "> Adding chunk: ColBERTv2\n",
      "performed well in all of these settin...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: References\n",
      "Stack Exchange Data Dump.\n",
      "Liefu Ai, ...\n",
      "> Adding chunk: References\n",
      "Stack Exchange Data Dump.\n",
      "Liefu Ai, ...\n",
      "> Adding chunk: References\n",
      "Stack Exchange Data Dump.\n",
      "Liefu Ai, ...\n",
      "> Adding chunk: References\n",
      "Stack Exchange Data Dump.\n",
      "Liefu Ai, ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: In\n",
      "Proceedings of the 43rd International ACM SI...\n",
      "> Adding chunk: In\n",
      "Proceedings of the 43rd International ACM SI...\n",
      "> Adding chunk: In\n",
      "Proceedings of the 43rd International ACM SI...\n",
      "> Adding chunk: In\n",
      "Proceedings of the 43rd International ACM SI...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Sebastian Hofstätter,\n",
      "Sophia Althammer,\n",
      "Michael...\n",
      "> Adding chunk: Sebastian Hofstätter,\n",
      "Sophia Althammer,\n",
      "Michael...\n",
      "> Adding chunk: Sebastian Hofstätter,\n",
      "Sophia Althammer,\n",
      "Michael...\n",
      "> Adding chunk: Sebastian Hofstätter,\n",
      "Sophia Althammer,\n",
      "Michael...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 2021a.\n",
      "Baleen: Robust Multi-Hop Reasoning at\n",
      "Sc...\n",
      "> Adding chunk: 2021a.\n",
      "Baleen: Robust Multi-Hop Reasoning at\n",
      "Sc...\n",
      "> Adding chunk: 2021a.\n",
      "Baleen: Robust Multi-Hop Reasoning at\n",
      "Sc...\n",
      "> Adding chunk: 2021a.\n",
      "Baleen: Robust Multi-Hop Reasoning at\n",
      "Sc...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin...\n",
      "> Adding chunk: Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin...\n",
      "> Adding chunk: Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin...\n",
      "> Adding chunk: Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: arXiv\n",
      "preprint\n",
      "arXiv:2107.13602.\n",
      "Ashwin Paranja...\n",
      "> Adding chunk: arXiv\n",
      "preprint\n",
      "arXiv:2107.13602.\n",
      "Ashwin Paranja...\n",
      "> Adding chunk: arXiv\n",
      "preprint\n",
      "arXiv:2107.13602.\n",
      "Ashwin Paranja...\n",
      "> Adding chunk: arXiv\n",
      "preprint\n",
      "arXiv:2107.13602.\n",
      "Ashwin Paranja...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Ellen Voorhees, Tasmeer Alam, Steven Bedrick, D...\n",
      "> Adding chunk: Ellen Voorhees, Tasmeer Alam, Steven Bedrick, D...\n",
      "> Adding chunk: Ellen Voorhees, Tasmeer Alam, Steven Bedrick, D...\n",
      "> Adding chunk: Ellen Voorhees, Tasmeer Alam, Steven Bedrick, D...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Association for Computational Linguistics.\n",
      "Ikuy...\n",
      "> Adding chunk: Association for Computational Linguistics.\n",
      "Ikuy...\n",
      "> Adding chunk: Association for Computational Linguistics.\n",
      "Ikuy...\n",
      "> Adding chunk: Association for Computational Linguistics.\n",
      "Ikuy...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: ColBERT\n",
      "Random\n",
      "1\n",
      "16\n",
      "256\n",
      "4096\n",
      "# Distinct Tokens ...\n",
      "> Adding chunk: ColBERT\n",
      "Random\n",
      "1\n",
      "16\n",
      "256\n",
      "4096\n",
      "# Distinct Tokens ...\n",
      "> Adding chunk: ColBERT\n",
      "Random\n",
      "1\n",
      "16\n",
      "256\n",
      "4096\n",
      "# Distinct Tokens ...\n",
      "> Adding chunk: ColBERT\n",
      "Random\n",
      "1\n",
      "16\n",
      "256\n",
      "4096\n",
      "# Distinct Tokens ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: In particular, when ap-\n",
      "plied to a vanilla ColB...\n",
      "> Adding chunk: In particular, when ap-\n",
      "plied to a vanilla ColB...\n",
      "> Adding chunk: In particular, when ap-\n",
      "plied to a vanilla ColB...\n",
      "> Adding chunk: In particular, when ap-\n",
      "plied to a vanilla ColB...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Cluster ID\n",
      "Most Common Tokens\n",
      "Most Common Clust...\n",
      "> Adding chunk: Cluster ID\n",
      "Most Common Tokens\n",
      "Most Common Clust...\n",
      "> Adding chunk: Cluster ID\n",
      "Most Common Tokens\n",
      "Most Common Clust...\n",
      "> Adding chunk: Cluster ID\n",
      "Most Common Tokens\n",
      "Most Common Clust...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Similarly, on the HoVer (Jiang et al., 2020) de...\n",
      "> Adding chunk: Similarly, on the HoVer (Jiang et al., 2020) de...\n",
      "> Adding chunk: Similarly, on the HoVer (Jiang et al., 2020) de...\n",
      "> Adding chunk: Similarly, on the HoVer (Jiang et al., 2020) de...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: All posts are written\n",
      "in English.\n",
      "Passages\n",
      "As m...\n",
      "> Adding chunk: All posts are written\n",
      "in English.\n",
      "Passages\n",
      "As m...\n",
      "> Adding chunk: All posts are written\n",
      "in English.\n",
      "Passages\n",
      "As m...\n",
      "> Adding chunk: All posts are written\n",
      "in English.\n",
      "Passages\n",
      "As m...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: 5\n",
      "10\n",
      "15\n",
      "20\n",
      "Answers per query\n",
      "[Forum] Pooled\n",
      "[Fo...\n",
      "> Adding chunk: 5\n",
      "10\n",
      "15\n",
      "20\n",
      "Answers per query\n",
      "[Forum] Pooled\n",
      "[Fo...\n",
      "> Adding chunk: 5\n",
      "10\n",
      "15\n",
      "20\n",
      "Answers per query\n",
      "[Forum] Pooled\n",
      "[Fo...\n",
      "> Adding chunk: 5\n",
      "10\n",
      "15\n",
      "20\n",
      "Answers per query\n",
      "[Forum] Pooled\n",
      "[Fo...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: The GooAQ dataset is licensed under\n",
      "an Apache l...\n",
      "> Adding chunk: The GooAQ dataset is licensed under\n",
      "an Apache l...\n",
      "> Adding chunk: The GooAQ dataset is licensed under\n",
      "an Apache l...\n",
      "> Adding chunk: The GooAQ dataset is licensed under\n",
      "an Apache l...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: k-means clustering,9 though unlike ColBERT we\n",
      "d...\n",
      "> Adding chunk: k-means clustering,9 though unlike ColBERT we\n",
      "d...\n",
      "> Adding chunk: k-means clustering,9 though unlike ColBERT we\n",
      "d...\n",
      "> Adding chunk: k-means clustering,9 though unlike ColBERT we\n",
      "d...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: This\n",
      "adapts the DPR (Karpukhin et al., 2020) ev...\n",
      "> Adding chunk: This\n",
      "adapts the DPR (Karpukhin et al., 2020) ev...\n",
      "> Adding chunk: This\n",
      "adapts the DPR (Karpukhin et al., 2020) ev...\n",
      "> Adding chunk: This\n",
      "adapts the DPR (Karpukhin et al., 2020) ev...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Topic\n",
      "Communities\n",
      "# Passages\n",
      "# Search queries\n",
      "#...\n",
      "> Adding chunk: Topic\n",
      "Communities\n",
      "# Passages\n",
      "# Search queries\n",
      "#...\n",
      "> Adding chunk: Topic\n",
      "Communities\n",
      "# Passages\n",
      "# Search queries\n",
      "#...\n",
      "> Adding chunk: Topic\n",
      "Communities\n",
      "# Passages\n",
      "# Search queries\n",
      "#...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.43s/it]\n",
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.56s/it]\n",
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.52s/it]\n",
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the document using PyMuPDFReader\n",
    "file = \"ColbertV2_2112.01488v3.pdf\"  # Replace with your actual file path\n",
    "reader = PyMuPDFReader()\n",
    "docs = reader.load(file)\n",
    "\n",
    "# Create the index\n",
    "index = VectorStoreIndex.from_documents(documents=docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1e3195d-34ed-4856-8ae0-7fd8c89de121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openaikey import key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = str(key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cc95994-1591-4f85-9e52-eb4a45bb91bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/config.json HTTP/11\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /colbert-ir/colbertv2.0/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    }
   ],
   "source": [
    "colbert_reranker = ColbertRerank(\n",
    "    top_n=5,\n",
    "    model=\"colbert-ir/colbertv2.0\",\n",
    "    tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "    keep_retrieval_score=True,\n",
    "    \n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[colbert_reranker],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7267565a-8ffb-479a-a4ec-f7a673b34816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.indices.utils:> Top 10 nodes:\n",
      "> [Node 667e6482-bade-419a-b454-f250400357ec] [Similarity score:             0.745617] Q: what is xerror in rpart?\n",
      "Q: is sub question one word?\n",
      "Q: how to open a garage door without mak...\n",
      "> [Node 7285a6da-978c-4ef2-a006-7ff228f57c53] [Similarity score:             0.738183] Similarly, on the HoVer (Jiang et al., 2020) dev\n",
      "set, Baleen’s retrieval R@100 dropped from 92.2%...\n",
      "> [Node 61ac8c3b-6fde-47f6-b04c-1391368ac740] [Similarity score:             0.734178] Sub-table (a) reports results on BEIR and sub-table (b) reports results on\n",
      "the Wikipedia Open QA ...\n",
      "> [Node 07adbdc9-f86e-44a6-a677-b9a5a9b1b585] [Similarity score:             0.71913] In particular, when ap-\n",
      "plied to a vanilla ColBERT model on MS MARCO\n",
      "whose MRR@10 is 36.2% and Re...\n",
      "> [Node acccae6d-cd0c-4d14-a15a-007c1e6009b0] [Similarity score:             0.709823] sages. This is known to lead to artiﬁcial lexical\n",
      "bias (Lee et al., 2019), where crowdworkers cop...\n",
      "> [Node d8688276-4aaf-4204-9dbb-18ed13e3a5df] [Similarity score:             0.705386] Dev-set results for baseline systems are from their re-\n",
      "spective papers: Zhan et al. (2020b), Xio...\n",
      "> [Node 2f27a95a-70ab-4233-865a-e226210953d0] [Similarity score:             0.703113] This\n",
      "adapts the DPR (Karpukhin et al., 2020) evaluation\n",
      "code.10 We use the preprocessed Wikipedia...\n",
      "> [Node a3201c46-8bfb-4989-aff7-a61194a095a4] [Similarity score:             0.698114] Corpus\n",
      "Models without Distillation\n",
      "Models with Distillation\n",
      "ColBERT\n",
      "DPR-M\n",
      "ANCE\n",
      "MoDIR\n",
      "TAS-B\n",
      "Rocket...\n",
      "> [Node 7192a93b-5adf-482b-a495-899e2db20c2f] [Similarity score:             0.697216] a cross-encoder and hard-negative mining (§3.2)\n",
      "to boost quality beyond any existing method, and\n",
      "...\n",
      "> [Node 8f702d58-ad67-4684-a7f7-8f675103216e] [Similarity score:             0.693991] In\n",
      "practice, user-facing IR and QA applications often\n",
      "pertain to domain-speciﬁc corpora, for whic...\n",
      "> Top 10 nodes:\n",
      "> [Node 667e6482-bade-419a-b454-f250400357ec] [Similarity score:             0.745617] Q: what is xerror in rpart?\n",
      "Q: is sub question one word?\n",
      "Q: how to open a garage door without mak...\n",
      "> [Node 7285a6da-978c-4ef2-a006-7ff228f57c53] [Similarity score:             0.738183] Similarly, on the HoVer (Jiang et al., 2020) dev\n",
      "set, Baleen’s retrieval R@100 dropped from 92.2%...\n",
      "> [Node 61ac8c3b-6fde-47f6-b04c-1391368ac740] [Similarity score:             0.734178] Sub-table (a) reports results on BEIR and sub-table (b) reports results on\n",
      "the Wikipedia Open QA ...\n",
      "> [Node 07adbdc9-f86e-44a6-a677-b9a5a9b1b585] [Similarity score:             0.71913] In particular, when ap-\n",
      "plied to a vanilla ColBERT model on MS MARCO\n",
      "whose MRR@10 is 36.2% and Re...\n",
      "> [Node acccae6d-cd0c-4d14-a15a-007c1e6009b0] [Similarity score:             0.709823] sages. This is known to lead to artiﬁcial lexical\n",
      "bias (Lee et al., 2019), where crowdworkers cop...\n",
      "> [Node d8688276-4aaf-4204-9dbb-18ed13e3a5df] [Similarity score:             0.705386] Dev-set results for baseline systems are from their re-\n",
      "spective papers: Zhan et al. (2020b), Xio...\n",
      "> [Node 2f27a95a-70ab-4233-865a-e226210953d0] [Similarity score:             0.703113] This\n",
      "adapts the DPR (Karpukhin et al., 2020) evaluation\n",
      "code.10 We use the preprocessed Wikipedia...\n",
      "> [Node a3201c46-8bfb-4989-aff7-a61194a095a4] [Similarity score:             0.698114] Corpus\n",
      "Models without Distillation\n",
      "Models with Distillation\n",
      "ColBERT\n",
      "DPR-M\n",
      "ANCE\n",
      "MoDIR\n",
      "TAS-B\n",
      "Rocket...\n",
      "> [Node 7192a93b-5adf-482b-a495-899e2db20c2f] [Similarity score:             0.697216] a cross-encoder and hard-negative mining (§3.2)\n",
      "to boost quality beyond any existing method, and\n",
      "...\n",
      "> [Node 8f702d58-ad67-4684-a7f7-8f675103216e] [Similarity score:             0.693991] In\n",
      "practice, user-facing IR and QA applications often\n",
      "pertain to domain-speciﬁc corpora, for whic...\n",
      "> Top 10 nodes:\n",
      "> [Node 667e6482-bade-419a-b454-f250400357ec] [Similarity score:             0.745617] Q: what is xerror in rpart?\n",
      "Q: is sub question one word?\n",
      "Q: how to open a garage door without mak...\n",
      "> [Node 7285a6da-978c-4ef2-a006-7ff228f57c53] [Similarity score:             0.738183] Similarly, on the HoVer (Jiang et al., 2020) dev\n",
      "set, Baleen’s retrieval R@100 dropped from 92.2%...\n",
      "> [Node 61ac8c3b-6fde-47f6-b04c-1391368ac740] [Similarity score:             0.734178] Sub-table (a) reports results on BEIR and sub-table (b) reports results on\n",
      "the Wikipedia Open QA ...\n",
      "> [Node 07adbdc9-f86e-44a6-a677-b9a5a9b1b585] [Similarity score:             0.71913] In particular, when ap-\n",
      "plied to a vanilla ColBERT model on MS MARCO\n",
      "whose MRR@10 is 36.2% and Re...\n",
      "> [Node acccae6d-cd0c-4d14-a15a-007c1e6009b0] [Similarity score:             0.709823] sages. This is known to lead to artiﬁcial lexical\n",
      "bias (Lee et al., 2019), where crowdworkers cop...\n",
      "> [Node d8688276-4aaf-4204-9dbb-18ed13e3a5df] [Similarity score:             0.705386] Dev-set results for baseline systems are from their re-\n",
      "spective papers: Zhan et al. (2020b), Xio...\n",
      "> [Node 2f27a95a-70ab-4233-865a-e226210953d0] [Similarity score:             0.703113] This\n",
      "adapts the DPR (Karpukhin et al., 2020) evaluation\n",
      "code.10 We use the preprocessed Wikipedia...\n",
      "> [Node a3201c46-8bfb-4989-aff7-a61194a095a4] [Similarity score:             0.698114] Corpus\n",
      "Models without Distillation\n",
      "Models with Distillation\n",
      "ColBERT\n",
      "DPR-M\n",
      "ANCE\n",
      "MoDIR\n",
      "TAS-B\n",
      "Rocket...\n",
      "> [Node 7192a93b-5adf-482b-a495-899e2db20c2f] [Similarity score:             0.697216] a cross-encoder and hard-negative mining (§3.2)\n",
      "to boost quality beyond any existing method, and\n",
      "...\n",
      "> [Node 8f702d58-ad67-4684-a7f7-8f675103216e] [Similarity score:             0.693991] In\n",
      "practice, user-facing IR and QA applications often\n",
      "pertain to domain-speciﬁc corpora, for whic...\n",
      "> Top 10 nodes:\n",
      "> [Node 667e6482-bade-419a-b454-f250400357ec] [Similarity score:             0.745617] Q: what is xerror in rpart?\n",
      "Q: is sub question one word?\n",
      "Q: how to open a garage door without mak...\n",
      "> [Node 7285a6da-978c-4ef2-a006-7ff228f57c53] [Similarity score:             0.738183] Similarly, on the HoVer (Jiang et al., 2020) dev\n",
      "set, Baleen’s retrieval R@100 dropped from 92.2%...\n",
      "> [Node 61ac8c3b-6fde-47f6-b04c-1391368ac740] [Similarity score:             0.734178] Sub-table (a) reports results on BEIR and sub-table (b) reports results on\n",
      "the Wikipedia Open QA ...\n",
      "> [Node 07adbdc9-f86e-44a6-a677-b9a5a9b1b585] [Similarity score:             0.71913] In particular, when ap-\n",
      "plied to a vanilla ColBERT model on MS MARCO\n",
      "whose MRR@10 is 36.2% and Re...\n",
      "> [Node acccae6d-cd0c-4d14-a15a-007c1e6009b0] [Similarity score:             0.709823] sages. This is known to lead to artiﬁcial lexical\n",
      "bias (Lee et al., 2019), where crowdworkers cop...\n",
      "> [Node d8688276-4aaf-4204-9dbb-18ed13e3a5df] [Similarity score:             0.705386] Dev-set results for baseline systems are from their re-\n",
      "spective papers: Zhan et al. (2020b), Xio...\n",
      "> [Node 2f27a95a-70ab-4233-865a-e226210953d0] [Similarity score:             0.703113] This\n",
      "adapts the DPR (Karpukhin et al., 2020) evaluation\n",
      "code.10 We use the preprocessed Wikipedia...\n",
      "> [Node a3201c46-8bfb-4989-aff7-a61194a095a4] [Similarity score:             0.698114] Corpus\n",
      "Models without Distillation\n",
      "Models with Distillation\n",
      "ColBERT\n",
      "DPR-M\n",
      "ANCE\n",
      "MoDIR\n",
      "TAS-B\n",
      "Rocket...\n",
      "> [Node 7192a93b-5adf-482b-a495-899e2db20c2f] [Similarity score:             0.697216] a cross-encoder and hard-negative mining (§3.2)\n",
      "to boost quality beyond any existing method, and\n",
      "...\n",
      "> [Node 8f702d58-ad67-4684-a7f7-8f675103216e] [Similarity score:             0.693991] In\n",
      "practice, user-facing IR and QA applications often\n",
      "pertain to domain-speciﬁc corpora, for whic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.705386269098783\\n\\nDev-set results for baseline systems are from their re-\\nspective papers: Zhan et al. (2020b), Xiong et al. (2020)\\nfor DPR and ANCE, Zhan et al. (2020a), Khattab and\\nZaharia (2020), Hofstätter et al. (2021), Gao and Callan\\n(2021), Ren et al. (2021a), Formal et al. (2021a), and\\nRen et al. (2021b).\\nPAIR, and RocketQAv2 to achieve higher qual-\\nity than vanilla ColBERT. These supervision gains\\nchallenge the value of ﬁne-grained late interaction,\\nand it is not inherently clear whether the stronger\\ninductive biases of ColBERT-like models permit it\\nto accept similar gains under distillation, especially\\nwhen using compressed representations. Despite\\nthis, we ﬁnd that with denoised supervision and\\nresidual compression, ColBERTv2 achieves the\\nhighest quality across all systems. As we discuss\\nin §5.3, it exhibits space footprint competitive with\\nthese single-vector models and much lower than\\nvanilla ColBERT.\\nBesides the ofﬁcial dev set, we evaluated Col-\\nBERTv2, SPLADEv2, and RocketQAv2 on the\\n“Local Eval” test set described by Khattab and Za-\\nharia (2020) for MS MARCO, which consists of\\n5000 queries disjoint with the training and the of-\\nﬁcial dev sets. These queries are obtained from\\nlabeled 50k queries that are provided in the ofﬁcial\\nMS MARCO Passage Ranking task as additional\\nvalidation data.4 On this test set, ColBERTv2 ob-\\ntains 40.8% MRR@10, considerably outperform-\\ning the baselines, including RocketQAv2 which\\nmakes use of document titles in addition to the\\npassage text unlike the other systems.\\n4These are sampled from delta between qrels.dev.tsv\\nand\\nqrels.dev.small.tsv\\non\\nhttps://microsoft.\\ngithub.io/msmarco/Datasets. We refer to Khattab and\\nZaharia (2020) for details. All our query IDs will be made\\npublic to aid reproducibility.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 3\\nretrieval_score: 0.6939909166632486\\n\\nIn\\npractice, user-facing IR and QA applications often\\npertain to domain-speciﬁc corpora, for which little\\nto no training data is available and whose topics\\nare under-represented in large public collections.\\nThis out-of-domain regime has received recent\\nattention with the BEIR (Thakur et al., 2021) bench-\\nmark. BEIR combines several existing datasets\\ninto a heterogeneous suite for “zero-shot IR” tasks,\\nspanning bio-medical, ﬁnancial, and scientiﬁc do-\\nmains. While the BEIR datasets provide a use-\\nful testbed, many capture broad semantic related-\\nness tasks—like citations, counter arguments, or\\nduplicate questions–instead of natural search tasks,\\nor else they focus on high-popularity entities like\\nthose in Wikipedia. In §4, we introduce LoTTE, a\\nnew dataset for out-of-domain retrieval, exhibiting\\nnatural search queries over long-tail topics.\\n3\\nColBERTv2\\nWe now introduce ColBERTv2, which improves\\nthe quality of multi-vector retrieval models (§3.2)\\nwhile reducing their space footprint (§3.3).\\n3.1\\nModeling\\nColBERTv2 adopts the late interaction architecture\\nof ColBERT, depicted in Figure 1. Queries and pas-\\nsages are independently encoded with BERT (De-\\nvlin et al., 2019), and the output embeddings encod-\\ning each token are projected to a lower dimension.\\nDuring ofﬂine indexing, every passage d in the\\ncorpus is encoded into a set of vectors, and these\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 2\\nretrieval_score: 0.6972163446277386\\n\\na cross-encoder and hard-negative mining (§3.2)\\nto boost quality beyond any existing method, and\\nthen uses a residual compression mechanism (§3.3)\\nto reduce the space footprint of late interaction by\\n6–10× while preserving quality. As a result, Col-\\nBERTv2 establishes state-of-the-art retrieval qual-\\nity both within and outside its training domain with\\na competitive space footprint with typical single-\\nvector models.\\nWhen trained on MS MARCO Passage Rank-\\ning, ColBERTv2 achieves the highest MRR@10 of\\nany standalone retriever. In addition to in-domain\\nquality, we seek a retriever that generalizes “zero-\\nshot” to domain-speciﬁc corpora and long-tail top-\\nics, ones that are often under-represented in large\\npublic training sets. To this end, we evaluate Col-\\nBERTv2 on a wide array of out-of-domain bench-\\nmarks. These include three Wikipedia Open-QA\\nretrieval tests and 13 diverse retrieval and semantic-\\nsimilarity tasks from BEIR (Thakur et al., 2021). In\\naddition, we introduce a new benchmark, dubbed\\nLoTTE, for Long-Tail Topic-stratiﬁed Evaluation\\nfor IR that features 12 domain-speciﬁc search\\ntests, spanning StackExchange communities and\\nusing queries from GooAQ (Khashabi et al., 2021).\\nLoTTE focuses on relatively long-tail topics in\\nits passages, unlike the Open-QA tests and many\\nof the BEIR tasks, and evaluates models on their\\ncapacity to answer natural search queries with a\\npractical intent, unlike many of BEIR’s semantic-\\nsimilarity tasks. On 22 of 28 out-of-domain tests,\\nColBERTv2 achieves the highest quality, outper-\\nforming the next best retriever by up to 8% relative\\ngain, while using its compressed representations.\\nThis work makes the following contributions:\\n1. We propose ColBERTv2, a retriever that com-\\nbines denoised supervision and residual com-\\npression, leveraging the token-level decom-\\nposition of late interaction to achieve high\\nrobustness with a reduced space footprint.\\n2. We introduce LoTTE, a new resource for out-\\nof-domain evaluation of retrievers. LoTTE fo-\\ncuses on natural information-seeking queries\\nover long-tail topics, an important yet under-\\nstudied application space.\\n3. We evaluate ColBERTv2 across a wide range\\nof settings, establishing state-of-the-art qual-\\nity within and outside the training domain.\\n2\\nBackground & Related Work\\n2.1\\nToken-Decomposed Scoring in Neural IR\\nMany neural IR approaches encode passages as\\na single high-dimensional vector, trading off the\\nhigher quality of cross-encoders for improved ef-\\nﬁciency and scalability (Karpukhin et al., 2020;\\nXiong et al., 2020; Qu et al., 2021).\\nCol-\\nBERT’s (Khattab and Zaharia, 2020) late inter-\\naction paradigm addresses this tradeoff by com-\\nputing multi-vector embeddings and using a scal-\\nable “MaxSim” operator for retrieval.\\nSeveral\\nother systems leverage multi-vector representa-\\ntions, including Poly-encoders (Humeau et al.,\\n2020), PreTTR (MacAvaney et al., 2020), and\\nMORES (Gao et al., 2020), but these target\\nattention-based re-ranking as opposed to Col-\\nBERT’s scalable MaxSim end-to-end retrieval.\\nME-BERT (Luan et al., 2021) generates token-\\nlevel document embeddings similar to ColBERT,\\nbut retains a single embedding vector for queries.\\nCOIL (Gao et al., 2021) also generates token-level\\ndocument embeddings, but the token interactions\\nare restricted to lexical matching between query\\nand document terms. uniCOIL (Lin and Ma, 2021)\\nlimits the token embedding vectors of COIL to a\\nsingle dimension, reducing them to scalar weights\\nthat extend models like DeepCT (Dai and Callan,\\n2020) and DeepImpact (Mallia et al., 2021). To\\nproduce scalar weights, SPLADE (Formal et al.,\\n2021b) and SPLADEv2 (Formal et al., 2021a) pro-\\nduce a sparse vocabulary-level vector that retains\\nthe term-level decomposition of late interaction\\nwhile simplifying the storage into one dimension\\nper token. The SPLADE family also piggybacks on\\nthe language modeling capacity acquired by BERT\\nduring pretraining.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 17\\nretrieval_score: 0.7381827551664437\\n\\nSimilarly, on the HoVer (Jiang et al., 2020) dev\\nset, Baleen’s retrieval R@100 dropped from 92.2%\\nto only 90.6% but its sentence-level exact match\\nremained roughly the same, going from 39.2% to\\n39.4%. We hypothesize that the supervision meth-\\nods applied in ColBERTv2 (§3.2) can also be ap-\\nplied to lift quality in downstream tasks by improv-\\ning the recall of retrieval for these tasks. We leave\\nsuch exploration for future work.\\nC\\nRetrieval Latency\\nFigure 3 evaluates the latency of ColBERTv2\\nacross three collections of varying sizes, namely,\\nMS MARCO, LoTTE Pooled (dev), and LoTTE\\nLifestyle (dev), which contain approximately 9M\\npassages, 2.4M answer posts, and 270k answer\\nposts, respectively. We average latency across three\\nruns of the MS MARCO dev set and the LoTTE\\n“search” queries. Search is executed using a Titan\\nV GPU on a server with two Intel Xeon Gold 6132\\nCPUs, each with 28 hardware execution contexts.\\nThe ﬁgure varies three settings of ColBERTv2.\\nIn particular, we evaluate indexing with 1-bit and\\n2-bit encoding (§3.4) and searching by probing the\\nnearest 1, 2, or 4 centroids to each query vector\\n(§3.5). When probing probe centroids per vector,\\nwe score either probe × 212 or probe × 214 candi-\\ndates per query.8\\nTo begin with, we notice that the quality reported\\non the x-axis varies only within a relatively narrow\\nrange. For instance, the axis ranges from 38.50\\nthrough 39.75 for MS MARCO, and all but two of\\nthe cheapest settings score above 39.00. Similarly,\\nthe y-axis varies between approximately 50 mil-\\nliseconds per query up to 250 milliseconds (mostly\\nunder 150 milliseconds) using our relatively simple\\nPython-based implementation.\\nDigging deeper, we see that the best quality\\nin these metrics can be achieved or approached\\nclosely with around 100 milliseconds of latency\\nacross all three datasets, despite their various sizes\\nand characteristics, and that 2-bit indexing reliably\\noutperforms 1-bit indexing but the loss from more\\naggressive compression is small.\\nD\\nLoTTE\\nDomain coverage\\nTable 9 presents the full dis-\\ntribution of communities in the LoTTE dev dataset.\\n8These settings are selected based on preliminary explo-\\nration of these parameters, which indicated that performance\\nfor larger probe values tends to require scoring a larger num-\\nber of candidates.\\n0\\n200\\n400\\n600\\nWords per passage\\nPooled\\nLifestyle\\nTechnology\\nScience\\nRecreation\\nWriting\\nFigure 4: LoTTE words per passage\\n5\\n10\\n15\\n20\\nWords per query\\n[Forum] Pooled\\n[Forum] Lifestyle\\n[Forum] Technology\\n[Forum] Science\\n[Forum] Recreation\\n[Forum] Writing\\n[Search] Pooled\\n[Search] Lifestyle\\n[Search] Technology\\n[Search] Science\\n[Search] Recreation\\n[Search] Writing\\nFigure 5: LoTTE words per query\\nThe topics covered by LoTTE cover a wide range\\nof linguistic phenomena given the diversity in top-\\nics and communities represented. However, since\\nall posts are submitted by anonymous users we do\\nnot have demographic information regarding the\\nidentify of the contributors. All posts are written\\nin English.\\nPassages\\nAs mentioned in §4, we construct\\nLoTTE collections by selecting passages from the\\nStackExchange archive with positive scores. We\\nremove HTML tags from passages and ﬁlter out\\nempty passages. For each passage we record its\\ncorresponding query and save the query-to-passage\\nmapping to keep track of the posted answers corre-\\nsponding to each query.\\nSearch queries\\nWe construct the list of LoTTE\\nsearch queries by drawing from GooAQ queries\\nthat appear in the StackExchange post archive. We\\nﬁrst shufﬂe the list of GooAQ queries so that in\\ncases where multiple queries exist for the same\\nanswer passage we randomly select the query to\\ninclude in LoTTE rather than always selecting the\\nﬁrst appearing query. We verify that every query\\nhas at least one corresponding answer passage.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.7456166799640073\\n\\nQ: what is xerror in rpart?\\nQ: is sub question one word?\\nQ: how to open a garage door without making noise?\\nQ:\\nis docx and dotx the same?\\nQ: are upvotes and downvotes\\nanonymous?\\nQ: what is the difference between descriptive\\nessay and narrative essay?\\nQ: how to change default\\nuser proﬁle in chrome?\\nQ: does autohotkey need to be\\ninstalled?\\nQ: how do you tag someone on facebook with\\na youtube video?\\nQ: has mjolnir ever been broken?\\nQ: Snoopy can balance on an edge atop his doghouse. Is any\\nreason given for this?\\nQ: How many Ents were at the\\nEntmoot?\\nQ: What does a hexagonal sun tell us about\\nthe camera lens/sensor?\\nQ: Should I simply ignore it if\\nauthors assume that Im male in their response to my review of\\ntheir article?\\nQ: Why is the 2s orbital lower in energy than\\nthe 2p orbital when the electrons in 2s are usually farther from\\nthe nucleus?\\nQ: Are there reasons to use colour ﬁlters\\nwith digital cameras?\\nQ: How does the current know how\\nmuch to ﬂow, before having seen the resistor?\\nQ: What\\nis the difference between Fact and Truth?\\nQ: hAs a DM,\\nhow can I handle my Druid spying on everything with Wild\\nshape as a spider?\\nQ: What does 1x1 convolution mean\\nin a neural network?\\nTable 3: Comparison of a random sample of search\\nqueries (top) vs. forum queries (bottom).\\n2021b), we evaluate retrieval quality by comput-\\ning the success@5 (S@5) metric. Speciﬁcally, we\\naward a point to the system for each query where\\nit ﬁnds an accepted or upvoted (score ≥1) answer\\nfrom the target page in the top-5 hits.\\nAppendix D reports on the breakdown of con-\\nstituent communities per topic, the construction\\nprocedure of LoTTE as well as licensing considera-\\ntions, and relevant statistics. Figures 5 and 6 quan-\\ntitatively compare the search and forum queries.\\n5\\nEvaluation\\nWe now evaluate ColBERTv2 on passage retrieval\\ntasks, testing its quality within the training domain\\n(§5.1) as well as outside the training domain in\\nzero-shot settings (§5.2). Unless otherwise stated,\\nwe compress ColBERTv2 embeddings to b = 2\\nbits per dimension in our evaluation.\\n5.1\\nIn-Domain Retrieval Quality\\nSimilar to related work, we train for IR tasks on MS\\nMARCO Passage Ranking (Nguyen et al., 2016).\\nWithin the training domain, our development-set re-\\nsults are shown in Table 4, comparing ColBERTv2\\nwith vanilla ColBERT as well as state-of-the-art\\nsingle-vector systems.\\nWhile ColBERT outperforms single-vector sys-\\ntems like RepBERT, ANCE, and even TAS-B, im-\\nprovements in supervision such as distillation from\\ncross-encoders enable systems like SPLADEv2,\\nMethod\\nOfﬁcial Dev (7k)\\nLocal Eval (5k)\\nMRR@10 R@50 R@1k MRR@10 R@50 R@1k\\nModels without Distillation or Special Pretraining\\nRepBERT\\n30.4\\n-\\n94.3\\n-\\n-\\n-\\nDPR\\n31.1\\n-\\n95.2\\n-\\n-\\n-\\nANCE\\n33.0\\n-\\n95.9\\n-\\n-\\n-\\nLTRe\\n34.1\\n-\\n96.2\\n-\\n-\\n-\\nColBERT\\n36.0\\n82.9\\n96.8\\n36.7\\n-\\n-\\nModels with Distillation or Special Pretraining\\nTAS-B\\n34.7\\n-\\n97.8\\n-\\n-\\n-\\nSPLADEv2\\n36.8\\n-\\n97.9\\n37.9\\n84.9\\n98.0\\nPAIR\\n37.9\\n86.4\\n98.2\\n-\\n-\\n-\\ncoCondenser\\n38.2\\n-\\n98.4\\n-\\n-\\n-\\nRocketQAv2\\n38.8\\n86.2\\n98.1\\n39.8\\n85.8\\n97.9\\nColBERTv2\\n39.7\\n86.8\\n98.4\\n40.8\\n86.3\\n98.3\\nTable 4: In-domain performance on the development\\nset of MS MARCO Passage Ranking as well the “Local\\nEval” test set described by Khattab and Zaharia (2020).\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: How is ColbertV2 better than Colbert ?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.705386269098783\\n\\nDev-set results for baseline systems are from their re-\\nspective papers: Zhan et al. (2020b), Xiong et al. (2020)\\nfor DPR and ANCE, Zhan et al. (2020a), Khattab and\\nZaharia (2020), Hofstätter et al. (2021), Gao and Callan\\n(2021), Ren et al. (2021a), Formal et al. (2021a), and\\nRen et al. (2021b).\\nPAIR, and RocketQAv2 to achieve higher qual-\\nity than vanilla ColBERT. These supervision gains\\nchallenge the value of ﬁne-grained late interaction,\\nand it is not inherently clear whether the stronger\\ninductive biases of ColBERT-like models permit it\\nto accept similar gains under distillation, especially\\nwhen using compressed representations. Despite\\nthis, we ﬁnd that with denoised supervision and\\nresidual compression, ColBERTv2 achieves the\\nhighest quality across all systems. As we discuss\\nin §5.3, it exhibits space footprint competitive with\\nthese single-vector models and much lower than\\nvanilla ColBERT.\\nBesides the ofﬁcial dev set, we evaluated Col-\\nBERTv2, SPLADEv2, and RocketQAv2 on the\\n“Local Eval” test set described by Khattab and Za-\\nharia (2020) for MS MARCO, which consists of\\n5000 queries disjoint with the training and the of-\\nﬁcial dev sets. These queries are obtained from\\nlabeled 50k queries that are provided in the ofﬁcial\\nMS MARCO Passage Ranking task as additional\\nvalidation data.4 On this test set, ColBERTv2 ob-\\ntains 40.8% MRR@10, considerably outperform-\\ning the baselines, including RocketQAv2 which\\nmakes use of document titles in addition to the\\npassage text unlike the other systems.\\n4These are sampled from delta between qrels.dev.tsv\\nand\\nqrels.dev.small.tsv\\non\\nhttps://microsoft.\\ngithub.io/msmarco/Datasets. We refer to Khattab and\\nZaharia (2020) for details. All our query IDs will be made\\npublic to aid reproducibility.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 3\\nretrieval_score: 0.6939909166632486\\n\\nIn\\npractice, user-facing IR and QA applications often\\npertain to domain-speciﬁc corpora, for which little\\nto no training data is available and whose topics\\nare under-represented in large public collections.\\nThis out-of-domain regime has received recent\\nattention with the BEIR (Thakur et al., 2021) bench-\\nmark. BEIR combines several existing datasets\\ninto a heterogeneous suite for “zero-shot IR” tasks,\\nspanning bio-medical, ﬁnancial, and scientiﬁc do-\\nmains. While the BEIR datasets provide a use-\\nful testbed, many capture broad semantic related-\\nness tasks—like citations, counter arguments, or\\nduplicate questions–instead of natural search tasks,\\nor else they focus on high-popularity entities like\\nthose in Wikipedia. In §4, we introduce LoTTE, a\\nnew dataset for out-of-domain retrieval, exhibiting\\nnatural search queries over long-tail topics.\\n3\\nColBERTv2\\nWe now introduce ColBERTv2, which improves\\nthe quality of multi-vector retrieval models (§3.2)\\nwhile reducing their space footprint (§3.3).\\n3.1\\nModeling\\nColBERTv2 adopts the late interaction architecture\\nof ColBERT, depicted in Figure 1. Queries and pas-\\nsages are independently encoded with BERT (De-\\nvlin et al., 2019), and the output embeddings encod-\\ning each token are projected to a lower dimension.\\nDuring ofﬂine indexing, every passage d in the\\ncorpus is encoded into a set of vectors, and these\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 2\\nretrieval_score: 0.6972163446277386\\n\\na cross-encoder and hard-negative mining (§3.2)\\nto boost quality beyond any existing method, and\\nthen uses a residual compression mechanism (§3.3)\\nto reduce the space footprint of late interaction by\\n6–10× while preserving quality. As a result, Col-\\nBERTv2 establishes state-of-the-art retrieval qual-\\nity both within and outside its training domain with\\na competitive space footprint with typical single-\\nvector models.\\nWhen trained on MS MARCO Passage Rank-\\ning, ColBERTv2 achieves the highest MRR@10 of\\nany standalone retriever. In addition to in-domain\\nquality, we seek a retriever that generalizes “zero-\\nshot” to domain-speciﬁc corpora and long-tail top-\\nics, ones that are often under-represented in large\\npublic training sets. To this end, we evaluate Col-\\nBERTv2 on a wide array of out-of-domain bench-\\nmarks. These include three Wikipedia Open-QA\\nretrieval tests and 13 diverse retrieval and semantic-\\nsimilarity tasks from BEIR (Thakur et al., 2021). In\\naddition, we introduce a new benchmark, dubbed\\nLoTTE, for Long-Tail Topic-stratiﬁed Evaluation\\nfor IR that features 12 domain-speciﬁc search\\ntests, spanning StackExchange communities and\\nusing queries from GooAQ (Khashabi et al., 2021).\\nLoTTE focuses on relatively long-tail topics in\\nits passages, unlike the Open-QA tests and many\\nof the BEIR tasks, and evaluates models on their\\ncapacity to answer natural search queries with a\\npractical intent, unlike many of BEIR’s semantic-\\nsimilarity tasks. On 22 of 28 out-of-domain tests,\\nColBERTv2 achieves the highest quality, outper-\\nforming the next best retriever by up to 8% relative\\ngain, while using its compressed representations.\\nThis work makes the following contributions:\\n1. We propose ColBERTv2, a retriever that com-\\nbines denoised supervision and residual com-\\npression, leveraging the token-level decom-\\nposition of late interaction to achieve high\\nrobustness with a reduced space footprint.\\n2. We introduce LoTTE, a new resource for out-\\nof-domain evaluation of retrievers. LoTTE fo-\\ncuses on natural information-seeking queries\\nover long-tail topics, an important yet under-\\nstudied application space.\\n3. We evaluate ColBERTv2 across a wide range\\nof settings, establishing state-of-the-art qual-\\nity within and outside the training domain.\\n2\\nBackground & Related Work\\n2.1\\nToken-Decomposed Scoring in Neural IR\\nMany neural IR approaches encode passages as\\na single high-dimensional vector, trading off the\\nhigher quality of cross-encoders for improved ef-\\nﬁciency and scalability (Karpukhin et al., 2020;\\nXiong et al., 2020; Qu et al., 2021).\\nCol-\\nBERT’s (Khattab and Zaharia, 2020) late inter-\\naction paradigm addresses this tradeoff by com-\\nputing multi-vector embeddings and using a scal-\\nable “MaxSim” operator for retrieval.\\nSeveral\\nother systems leverage multi-vector representa-\\ntions, including Poly-encoders (Humeau et al.,\\n2020), PreTTR (MacAvaney et al., 2020), and\\nMORES (Gao et al., 2020), but these target\\nattention-based re-ranking as opposed to Col-\\nBERT’s scalable MaxSim end-to-end retrieval.\\nME-BERT (Luan et al., 2021) generates token-\\nlevel document embeddings similar to ColBERT,\\nbut retains a single embedding vector for queries.\\nCOIL (Gao et al., 2021) also generates token-level\\ndocument embeddings, but the token interactions\\nare restricted to lexical matching between query\\nand document terms. uniCOIL (Lin and Ma, 2021)\\nlimits the token embedding vectors of COIL to a\\nsingle dimension, reducing them to scalar weights\\nthat extend models like DeepCT (Dai and Callan,\\n2020) and DeepImpact (Mallia et al., 2021). To\\nproduce scalar weights, SPLADE (Formal et al.,\\n2021b) and SPLADEv2 (Formal et al., 2021a) pro-\\nduce a sparse vocabulary-level vector that retains\\nthe term-level decomposition of late interaction\\nwhile simplifying the storage into one dimension\\nper token. The SPLADE family also piggybacks on\\nthe language modeling capacity acquired by BERT\\nduring pretraining.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 17\\nretrieval_score: 0.7381827551664437\\n\\nSimilarly, on the HoVer (Jiang et al., 2020) dev\\nset, Baleen’s retrieval R@100 dropped from 92.2%\\nto only 90.6% but its sentence-level exact match\\nremained roughly the same, going from 39.2% to\\n39.4%. We hypothesize that the supervision meth-\\nods applied in ColBERTv2 (§3.2) can also be ap-\\nplied to lift quality in downstream tasks by improv-\\ning the recall of retrieval for these tasks. We leave\\nsuch exploration for future work.\\nC\\nRetrieval Latency\\nFigure 3 evaluates the latency of ColBERTv2\\nacross three collections of varying sizes, namely,\\nMS MARCO, LoTTE Pooled (dev), and LoTTE\\nLifestyle (dev), which contain approximately 9M\\npassages, 2.4M answer posts, and 270k answer\\nposts, respectively. We average latency across three\\nruns of the MS MARCO dev set and the LoTTE\\n“search” queries. Search is executed using a Titan\\nV GPU on a server with two Intel Xeon Gold 6132\\nCPUs, each with 28 hardware execution contexts.\\nThe ﬁgure varies three settings of ColBERTv2.\\nIn particular, we evaluate indexing with 1-bit and\\n2-bit encoding (§3.4) and searching by probing the\\nnearest 1, 2, or 4 centroids to each query vector\\n(§3.5). When probing probe centroids per vector,\\nwe score either probe × 212 or probe × 214 candi-\\ndates per query.8\\nTo begin with, we notice that the quality reported\\non the x-axis varies only within a relatively narrow\\nrange. For instance, the axis ranges from 38.50\\nthrough 39.75 for MS MARCO, and all but two of\\nthe cheapest settings score above 39.00. Similarly,\\nthe y-axis varies between approximately 50 mil-\\nliseconds per query up to 250 milliseconds (mostly\\nunder 150 milliseconds) using our relatively simple\\nPython-based implementation.\\nDigging deeper, we see that the best quality\\nin these metrics can be achieved or approached\\nclosely with around 100 milliseconds of latency\\nacross all three datasets, despite their various sizes\\nand characteristics, and that 2-bit indexing reliably\\noutperforms 1-bit indexing but the loss from more\\naggressive compression is small.\\nD\\nLoTTE\\nDomain coverage\\nTable 9 presents the full dis-\\ntribution of communities in the LoTTE dev dataset.\\n8These settings are selected based on preliminary explo-\\nration of these parameters, which indicated that performance\\nfor larger probe values tends to require scoring a larger num-\\nber of candidates.\\n0\\n200\\n400\\n600\\nWords per passage\\nPooled\\nLifestyle\\nTechnology\\nScience\\nRecreation\\nWriting\\nFigure 4: LoTTE words per passage\\n5\\n10\\n15\\n20\\nWords per query\\n[Forum] Pooled\\n[Forum] Lifestyle\\n[Forum] Technology\\n[Forum] Science\\n[Forum] Recreation\\n[Forum] Writing\\n[Search] Pooled\\n[Search] Lifestyle\\n[Search] Technology\\n[Search] Science\\n[Search] Recreation\\n[Search] Writing\\nFigure 5: LoTTE words per query\\nThe topics covered by LoTTE cover a wide range\\nof linguistic phenomena given the diversity in top-\\nics and communities represented. However, since\\nall posts are submitted by anonymous users we do\\nnot have demographic information regarding the\\nidentify of the contributors. All posts are written\\nin English.\\nPassages\\nAs mentioned in §4, we construct\\nLoTTE collections by selecting passages from the\\nStackExchange archive with positive scores. We\\nremove HTML tags from passages and ﬁlter out\\nempty passages. For each passage we record its\\ncorresponding query and save the query-to-passage\\nmapping to keep track of the posted answers corre-\\nsponding to each query.\\nSearch queries\\nWe construct the list of LoTTE\\nsearch queries by drawing from GooAQ queries\\nthat appear in the StackExchange post archive. We\\nﬁrst shufﬂe the list of GooAQ queries so that in\\ncases where multiple queries exist for the same\\nanswer passage we randomly select the query to\\ninclude in LoTTE rather than always selecting the\\nﬁrst appearing query. We verify that every query\\nhas at least one corresponding answer passage.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.7456166799640073\\n\\nQ: what is xerror in rpart?\\nQ: is sub question one word?\\nQ: how to open a garage door without making noise?\\nQ:\\nis docx and dotx the same?\\nQ: are upvotes and downvotes\\nanonymous?\\nQ: what is the difference between descriptive\\nessay and narrative essay?\\nQ: how to change default\\nuser proﬁle in chrome?\\nQ: does autohotkey need to be\\ninstalled?\\nQ: how do you tag someone on facebook with\\na youtube video?\\nQ: has mjolnir ever been broken?\\nQ: Snoopy can balance on an edge atop his doghouse. Is any\\nreason given for this?\\nQ: How many Ents were at the\\nEntmoot?\\nQ: What does a hexagonal sun tell us about\\nthe camera lens/sensor?\\nQ: Should I simply ignore it if\\nauthors assume that Im male in their response to my review of\\ntheir article?\\nQ: Why is the 2s orbital lower in energy than\\nthe 2p orbital when the electrons in 2s are usually farther from\\nthe nucleus?\\nQ: Are there reasons to use colour ﬁlters\\nwith digital cameras?\\nQ: How does the current know how\\nmuch to ﬂow, before having seen the resistor?\\nQ: What\\nis the difference between Fact and Truth?\\nQ: hAs a DM,\\nhow can I handle my Druid spying on everything with Wild\\nshape as a spider?\\nQ: What does 1x1 convolution mean\\nin a neural network?\\nTable 3: Comparison of a random sample of search\\nqueries (top) vs. forum queries (bottom).\\n2021b), we evaluate retrieval quality by comput-\\ning the success@5 (S@5) metric. Speciﬁcally, we\\naward a point to the system for each query where\\nit ﬁnds an accepted or upvoted (score ≥1) answer\\nfrom the target page in the top-5 hits.\\nAppendix D reports on the breakdown of con-\\nstituent communities per topic, the construction\\nprocedure of LoTTE as well as licensing considera-\\ntions, and relevant statistics. Figures 5 and 6 quan-\\ntitatively compare the search and forum queries.\\n5\\nEvaluation\\nWe now evaluate ColBERTv2 on passage retrieval\\ntasks, testing its quality within the training domain\\n(§5.1) as well as outside the training domain in\\nzero-shot settings (§5.2). Unless otherwise stated,\\nwe compress ColBERTv2 embeddings to b = 2\\nbits per dimension in our evaluation.\\n5.1\\nIn-Domain Retrieval Quality\\nSimilar to related work, we train for IR tasks on MS\\nMARCO Passage Ranking (Nguyen et al., 2016).\\nWithin the training domain, our development-set re-\\nsults are shown in Table 4, comparing ColBERTv2\\nwith vanilla ColBERT as well as state-of-the-art\\nsingle-vector systems.\\nWhile ColBERT outperforms single-vector sys-\\ntems like RepBERT, ANCE, and even TAS-B, im-\\nprovements in supervision such as distillation from\\ncross-encoders enable systems like SPLADEv2,\\nMethod\\nOfﬁcial Dev (7k)\\nLocal Eval (5k)\\nMRR@10 R@50 R@1k MRR@10 R@50 R@1k\\nModels without Distillation or Special Pretraining\\nRepBERT\\n30.4\\n-\\n94.3\\n-\\n-\\n-\\nDPR\\n31.1\\n-\\n95.2\\n-\\n-\\n-\\nANCE\\n33.0\\n-\\n95.9\\n-\\n-\\n-\\nLTRe\\n34.1\\n-\\n96.2\\n-\\n-\\n-\\nColBERT\\n36.0\\n82.9\\n96.8\\n36.7\\n-\\n-\\nModels with Distillation or Special Pretraining\\nTAS-B\\n34.7\\n-\\n97.8\\n-\\n-\\n-\\nSPLADEv2\\n36.8\\n-\\n97.9\\n37.9\\n84.9\\n98.0\\nPAIR\\n37.9\\n86.4\\n98.2\\n-\\n-\\n-\\ncoCondenser\\n38.2\\n-\\n98.4\\n-\\n-\\n-\\nRocketQAv2\\n38.8\\n86.2\\n98.1\\n39.8\\n85.8\\n97.9\\nColBERTv2\\n39.7\\n86.8\\n98.4\\n40.8\\n86.3\\n98.3\\nTable 4: In-domain performance on the development\\nset of MS MARCO Passage Ranking as well the “Local\\nEval” test set described by Khattab and Zaharia (2020).\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: How is ColbertV2 better than Colbert ?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.705386269098783\\n\\nDev-set results for baseline systems are from their re-\\nspective papers: Zhan et al. (2020b), Xiong et al. (2020)\\nfor DPR and ANCE, Zhan et al. (2020a), Khattab and\\nZaharia (2020), Hofstätter et al. (2021), Gao and Callan\\n(2021), Ren et al. (2021a), Formal et al. (2021a), and\\nRen et al. (2021b).\\nPAIR, and RocketQAv2 to achieve higher qual-\\nity than vanilla ColBERT. These supervision gains\\nchallenge the value of ﬁne-grained late interaction,\\nand it is not inherently clear whether the stronger\\ninductive biases of ColBERT-like models permit it\\nto accept similar gains under distillation, especially\\nwhen using compressed representations. Despite\\nthis, we ﬁnd that with denoised supervision and\\nresidual compression, ColBERTv2 achieves the\\nhighest quality across all systems. As we discuss\\nin §5.3, it exhibits space footprint competitive with\\nthese single-vector models and much lower than\\nvanilla ColBERT.\\nBesides the ofﬁcial dev set, we evaluated Col-\\nBERTv2, SPLADEv2, and RocketQAv2 on the\\n“Local Eval” test set described by Khattab and Za-\\nharia (2020) for MS MARCO, which consists of\\n5000 queries disjoint with the training and the of-\\nﬁcial dev sets. These queries are obtained from\\nlabeled 50k queries that are provided in the ofﬁcial\\nMS MARCO Passage Ranking task as additional\\nvalidation data.4 On this test set, ColBERTv2 ob-\\ntains 40.8% MRR@10, considerably outperform-\\ning the baselines, including RocketQAv2 which\\nmakes use of document titles in addition to the\\npassage text unlike the other systems.\\n4These are sampled from delta between qrels.dev.tsv\\nand\\nqrels.dev.small.tsv\\non\\nhttps://microsoft.\\ngithub.io/msmarco/Datasets. We refer to Khattab and\\nZaharia (2020) for details. All our query IDs will be made\\npublic to aid reproducibility.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 3\\nretrieval_score: 0.6939909166632486\\n\\nIn\\npractice, user-facing IR and QA applications often\\npertain to domain-speciﬁc corpora, for which little\\nto no training data is available and whose topics\\nare under-represented in large public collections.\\nThis out-of-domain regime has received recent\\nattention with the BEIR (Thakur et al., 2021) bench-\\nmark. BEIR combines several existing datasets\\ninto a heterogeneous suite for “zero-shot IR” tasks,\\nspanning bio-medical, ﬁnancial, and scientiﬁc do-\\nmains. While the BEIR datasets provide a use-\\nful testbed, many capture broad semantic related-\\nness tasks—like citations, counter arguments, or\\nduplicate questions–instead of natural search tasks,\\nor else they focus on high-popularity entities like\\nthose in Wikipedia. In §4, we introduce LoTTE, a\\nnew dataset for out-of-domain retrieval, exhibiting\\nnatural search queries over long-tail topics.\\n3\\nColBERTv2\\nWe now introduce ColBERTv2, which improves\\nthe quality of multi-vector retrieval models (§3.2)\\nwhile reducing their space footprint (§3.3).\\n3.1\\nModeling\\nColBERTv2 adopts the late interaction architecture\\nof ColBERT, depicted in Figure 1. Queries and pas-\\nsages are independently encoded with BERT (De-\\nvlin et al., 2019), and the output embeddings encod-\\ning each token are projected to a lower dimension.\\nDuring ofﬂine indexing, every passage d in the\\ncorpus is encoded into a set of vectors, and these\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 2\\nretrieval_score: 0.6972163446277386\\n\\na cross-encoder and hard-negative mining (§3.2)\\nto boost quality beyond any existing method, and\\nthen uses a residual compression mechanism (§3.3)\\nto reduce the space footprint of late interaction by\\n6–10× while preserving quality. As a result, Col-\\nBERTv2 establishes state-of-the-art retrieval qual-\\nity both within and outside its training domain with\\na competitive space footprint with typical single-\\nvector models.\\nWhen trained on MS MARCO Passage Rank-\\ning, ColBERTv2 achieves the highest MRR@10 of\\nany standalone retriever. In addition to in-domain\\nquality, we seek a retriever that generalizes “zero-\\nshot” to domain-speciﬁc corpora and long-tail top-\\nics, ones that are often under-represented in large\\npublic training sets. To this end, we evaluate Col-\\nBERTv2 on a wide array of out-of-domain bench-\\nmarks. These include three Wikipedia Open-QA\\nretrieval tests and 13 diverse retrieval and semantic-\\nsimilarity tasks from BEIR (Thakur et al., 2021). In\\naddition, we introduce a new benchmark, dubbed\\nLoTTE, for Long-Tail Topic-stratiﬁed Evaluation\\nfor IR that features 12 domain-speciﬁc search\\ntests, spanning StackExchange communities and\\nusing queries from GooAQ (Khashabi et al., 2021).\\nLoTTE focuses on relatively long-tail topics in\\nits passages, unlike the Open-QA tests and many\\nof the BEIR tasks, and evaluates models on their\\ncapacity to answer natural search queries with a\\npractical intent, unlike many of BEIR’s semantic-\\nsimilarity tasks. On 22 of 28 out-of-domain tests,\\nColBERTv2 achieves the highest quality, outper-\\nforming the next best retriever by up to 8% relative\\ngain, while using its compressed representations.\\nThis work makes the following contributions:\\n1. We propose ColBERTv2, a retriever that com-\\nbines denoised supervision and residual com-\\npression, leveraging the token-level decom-\\nposition of late interaction to achieve high\\nrobustness with a reduced space footprint.\\n2. We introduce LoTTE, a new resource for out-\\nof-domain evaluation of retrievers. LoTTE fo-\\ncuses on natural information-seeking queries\\nover long-tail topics, an important yet under-\\nstudied application space.\\n3. We evaluate ColBERTv2 across a wide range\\nof settings, establishing state-of-the-art qual-\\nity within and outside the training domain.\\n2\\nBackground & Related Work\\n2.1\\nToken-Decomposed Scoring in Neural IR\\nMany neural IR approaches encode passages as\\na single high-dimensional vector, trading off the\\nhigher quality of cross-encoders for improved ef-\\nﬁciency and scalability (Karpukhin et al., 2020;\\nXiong et al., 2020; Qu et al., 2021).\\nCol-\\nBERT’s (Khattab and Zaharia, 2020) late inter-\\naction paradigm addresses this tradeoff by com-\\nputing multi-vector embeddings and using a scal-\\nable “MaxSim” operator for retrieval.\\nSeveral\\nother systems leverage multi-vector representa-\\ntions, including Poly-encoders (Humeau et al.,\\n2020), PreTTR (MacAvaney et al., 2020), and\\nMORES (Gao et al., 2020), but these target\\nattention-based re-ranking as opposed to Col-\\nBERT’s scalable MaxSim end-to-end retrieval.\\nME-BERT (Luan et al., 2021) generates token-\\nlevel document embeddings similar to ColBERT,\\nbut retains a single embedding vector for queries.\\nCOIL (Gao et al., 2021) also generates token-level\\ndocument embeddings, but the token interactions\\nare restricted to lexical matching between query\\nand document terms. uniCOIL (Lin and Ma, 2021)\\nlimits the token embedding vectors of COIL to a\\nsingle dimension, reducing them to scalar weights\\nthat extend models like DeepCT (Dai and Callan,\\n2020) and DeepImpact (Mallia et al., 2021). To\\nproduce scalar weights, SPLADE (Formal et al.,\\n2021b) and SPLADEv2 (Formal et al., 2021a) pro-\\nduce a sparse vocabulary-level vector that retains\\nthe term-level decomposition of late interaction\\nwhile simplifying the storage into one dimension\\nper token. The SPLADE family also piggybacks on\\nthe language modeling capacity acquired by BERT\\nduring pretraining.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 17\\nretrieval_score: 0.7381827551664437\\n\\nSimilarly, on the HoVer (Jiang et al., 2020) dev\\nset, Baleen’s retrieval R@100 dropped from 92.2%\\nto only 90.6% but its sentence-level exact match\\nremained roughly the same, going from 39.2% to\\n39.4%. We hypothesize that the supervision meth-\\nods applied in ColBERTv2 (§3.2) can also be ap-\\nplied to lift quality in downstream tasks by improv-\\ning the recall of retrieval for these tasks. We leave\\nsuch exploration for future work.\\nC\\nRetrieval Latency\\nFigure 3 evaluates the latency of ColBERTv2\\nacross three collections of varying sizes, namely,\\nMS MARCO, LoTTE Pooled (dev), and LoTTE\\nLifestyle (dev), which contain approximately 9M\\npassages, 2.4M answer posts, and 270k answer\\nposts, respectively. We average latency across three\\nruns of the MS MARCO dev set and the LoTTE\\n“search” queries. Search is executed using a Titan\\nV GPU on a server with two Intel Xeon Gold 6132\\nCPUs, each with 28 hardware execution contexts.\\nThe ﬁgure varies three settings of ColBERTv2.\\nIn particular, we evaluate indexing with 1-bit and\\n2-bit encoding (§3.4) and searching by probing the\\nnearest 1, 2, or 4 centroids to each query vector\\n(§3.5). When probing probe centroids per vector,\\nwe score either probe × 212 or probe × 214 candi-\\ndates per query.8\\nTo begin with, we notice that the quality reported\\non the x-axis varies only within a relatively narrow\\nrange. For instance, the axis ranges from 38.50\\nthrough 39.75 for MS MARCO, and all but two of\\nthe cheapest settings score above 39.00. Similarly,\\nthe y-axis varies between approximately 50 mil-\\nliseconds per query up to 250 milliseconds (mostly\\nunder 150 milliseconds) using our relatively simple\\nPython-based implementation.\\nDigging deeper, we see that the best quality\\nin these metrics can be achieved or approached\\nclosely with around 100 milliseconds of latency\\nacross all three datasets, despite their various sizes\\nand characteristics, and that 2-bit indexing reliably\\noutperforms 1-bit indexing but the loss from more\\naggressive compression is small.\\nD\\nLoTTE\\nDomain coverage\\nTable 9 presents the full dis-\\ntribution of communities in the LoTTE dev dataset.\\n8These settings are selected based on preliminary explo-\\nration of these parameters, which indicated that performance\\nfor larger probe values tends to require scoring a larger num-\\nber of candidates.\\n0\\n200\\n400\\n600\\nWords per passage\\nPooled\\nLifestyle\\nTechnology\\nScience\\nRecreation\\nWriting\\nFigure 4: LoTTE words per passage\\n5\\n10\\n15\\n20\\nWords per query\\n[Forum] Pooled\\n[Forum] Lifestyle\\n[Forum] Technology\\n[Forum] Science\\n[Forum] Recreation\\n[Forum] Writing\\n[Search] Pooled\\n[Search] Lifestyle\\n[Search] Technology\\n[Search] Science\\n[Search] Recreation\\n[Search] Writing\\nFigure 5: LoTTE words per query\\nThe topics covered by LoTTE cover a wide range\\nof linguistic phenomena given the diversity in top-\\nics and communities represented. However, since\\nall posts are submitted by anonymous users we do\\nnot have demographic information regarding the\\nidentify of the contributors. All posts are written\\nin English.\\nPassages\\nAs mentioned in §4, we construct\\nLoTTE collections by selecting passages from the\\nStackExchange archive with positive scores. We\\nremove HTML tags from passages and ﬁlter out\\nempty passages. For each passage we record its\\ncorresponding query and save the query-to-passage\\nmapping to keep track of the posted answers corre-\\nsponding to each query.\\nSearch queries\\nWe construct the list of LoTTE\\nsearch queries by drawing from GooAQ queries\\nthat appear in the StackExchange post archive. We\\nﬁrst shufﬂe the list of GooAQ queries so that in\\ncases where multiple queries exist for the same\\nanswer passage we randomly select the query to\\ninclude in LoTTE rather than always selecting the\\nﬁrst appearing query. We verify that every query\\nhas at least one corresponding answer passage.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.7456166799640073\\n\\nQ: what is xerror in rpart?\\nQ: is sub question one word?\\nQ: how to open a garage door without making noise?\\nQ:\\nis docx and dotx the same?\\nQ: are upvotes and downvotes\\nanonymous?\\nQ: what is the difference between descriptive\\nessay and narrative essay?\\nQ: how to change default\\nuser proﬁle in chrome?\\nQ: does autohotkey need to be\\ninstalled?\\nQ: how do you tag someone on facebook with\\na youtube video?\\nQ: has mjolnir ever been broken?\\nQ: Snoopy can balance on an edge atop his doghouse. Is any\\nreason given for this?\\nQ: How many Ents were at the\\nEntmoot?\\nQ: What does a hexagonal sun tell us about\\nthe camera lens/sensor?\\nQ: Should I simply ignore it if\\nauthors assume that Im male in their response to my review of\\ntheir article?\\nQ: Why is the 2s orbital lower in energy than\\nthe 2p orbital when the electrons in 2s are usually farther from\\nthe nucleus?\\nQ: Are there reasons to use colour ﬁlters\\nwith digital cameras?\\nQ: How does the current know how\\nmuch to ﬂow, before having seen the resistor?\\nQ: What\\nis the difference between Fact and Truth?\\nQ: hAs a DM,\\nhow can I handle my Druid spying on everything with Wild\\nshape as a spider?\\nQ: What does 1x1 convolution mean\\nin a neural network?\\nTable 3: Comparison of a random sample of search\\nqueries (top) vs. forum queries (bottom).\\n2021b), we evaluate retrieval quality by comput-\\ning the success@5 (S@5) metric. Speciﬁcally, we\\naward a point to the system for each query where\\nit ﬁnds an accepted or upvoted (score ≥1) answer\\nfrom the target page in the top-5 hits.\\nAppendix D reports on the breakdown of con-\\nstituent communities per topic, the construction\\nprocedure of LoTTE as well as licensing considera-\\ntions, and relevant statistics. Figures 5 and 6 quan-\\ntitatively compare the search and forum queries.\\n5\\nEvaluation\\nWe now evaluate ColBERTv2 on passage retrieval\\ntasks, testing its quality within the training domain\\n(§5.1) as well as outside the training domain in\\nzero-shot settings (§5.2). Unless otherwise stated,\\nwe compress ColBERTv2 embeddings to b = 2\\nbits per dimension in our evaluation.\\n5.1\\nIn-Domain Retrieval Quality\\nSimilar to related work, we train for IR tasks on MS\\nMARCO Passage Ranking (Nguyen et al., 2016).\\nWithin the training domain, our development-set re-\\nsults are shown in Table 4, comparing ColBERTv2\\nwith vanilla ColBERT as well as state-of-the-art\\nsingle-vector systems.\\nWhile ColBERT outperforms single-vector sys-\\ntems like RepBERT, ANCE, and even TAS-B, im-\\nprovements in supervision such as distillation from\\ncross-encoders enable systems like SPLADEv2,\\nMethod\\nOfﬁcial Dev (7k)\\nLocal Eval (5k)\\nMRR@10 R@50 R@1k MRR@10 R@50 R@1k\\nModels without Distillation or Special Pretraining\\nRepBERT\\n30.4\\n-\\n94.3\\n-\\n-\\n-\\nDPR\\n31.1\\n-\\n95.2\\n-\\n-\\n-\\nANCE\\n33.0\\n-\\n95.9\\n-\\n-\\n-\\nLTRe\\n34.1\\n-\\n96.2\\n-\\n-\\n-\\nColBERT\\n36.0\\n82.9\\n96.8\\n36.7\\n-\\n-\\nModels with Distillation or Special Pretraining\\nTAS-B\\n34.7\\n-\\n97.8\\n-\\n-\\n-\\nSPLADEv2\\n36.8\\n-\\n97.9\\n37.9\\n84.9\\n98.0\\nPAIR\\n37.9\\n86.4\\n98.2\\n-\\n-\\n-\\ncoCondenser\\n38.2\\n-\\n98.4\\n-\\n-\\n-\\nRocketQAv2\\n38.8\\n86.2\\n98.1\\n39.8\\n85.8\\n97.9\\nColBERTv2\\n39.7\\n86.8\\n98.4\\n40.8\\n86.3\\n98.3\\nTable 4: In-domain performance on the development\\nset of MS MARCO Passage Ranking as well the “Local\\nEval” test set described by Khattab and Zaharia (2020).\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: How is ColbertV2 better than Colbert ?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.705386269098783\\n\\nDev-set results for baseline systems are from their re-\\nspective papers: Zhan et al. (2020b), Xiong et al. (2020)\\nfor DPR and ANCE, Zhan et al. (2020a), Khattab and\\nZaharia (2020), Hofstätter et al. (2021), Gao and Callan\\n(2021), Ren et al. (2021a), Formal et al. (2021a), and\\nRen et al. (2021b).\\nPAIR, and RocketQAv2 to achieve higher qual-\\nity than vanilla ColBERT. These supervision gains\\nchallenge the value of ﬁne-grained late interaction,\\nand it is not inherently clear whether the stronger\\ninductive biases of ColBERT-like models permit it\\nto accept similar gains under distillation, especially\\nwhen using compressed representations. Despite\\nthis, we ﬁnd that with denoised supervision and\\nresidual compression, ColBERTv2 achieves the\\nhighest quality across all systems. As we discuss\\nin §5.3, it exhibits space footprint competitive with\\nthese single-vector models and much lower than\\nvanilla ColBERT.\\nBesides the ofﬁcial dev set, we evaluated Col-\\nBERTv2, SPLADEv2, and RocketQAv2 on the\\n“Local Eval” test set described by Khattab and Za-\\nharia (2020) for MS MARCO, which consists of\\n5000 queries disjoint with the training and the of-\\nﬁcial dev sets. These queries are obtained from\\nlabeled 50k queries that are provided in the ofﬁcial\\nMS MARCO Passage Ranking task as additional\\nvalidation data.4 On this test set, ColBERTv2 ob-\\ntains 40.8% MRR@10, considerably outperform-\\ning the baselines, including RocketQAv2 which\\nmakes use of document titles in addition to the\\npassage text unlike the other systems.\\n4These are sampled from delta between qrels.dev.tsv\\nand\\nqrels.dev.small.tsv\\non\\nhttps://microsoft.\\ngithub.io/msmarco/Datasets. We refer to Khattab and\\nZaharia (2020) for details. All our query IDs will be made\\npublic to aid reproducibility.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 3\\nretrieval_score: 0.6939909166632486\\n\\nIn\\npractice, user-facing IR and QA applications often\\npertain to domain-speciﬁc corpora, for which little\\nto no training data is available and whose topics\\nare under-represented in large public collections.\\nThis out-of-domain regime has received recent\\nattention with the BEIR (Thakur et al., 2021) bench-\\nmark. BEIR combines several existing datasets\\ninto a heterogeneous suite for “zero-shot IR” tasks,\\nspanning bio-medical, ﬁnancial, and scientiﬁc do-\\nmains. While the BEIR datasets provide a use-\\nful testbed, many capture broad semantic related-\\nness tasks—like citations, counter arguments, or\\nduplicate questions–instead of natural search tasks,\\nor else they focus on high-popularity entities like\\nthose in Wikipedia. In §4, we introduce LoTTE, a\\nnew dataset for out-of-domain retrieval, exhibiting\\nnatural search queries over long-tail topics.\\n3\\nColBERTv2\\nWe now introduce ColBERTv2, which improves\\nthe quality of multi-vector retrieval models (§3.2)\\nwhile reducing their space footprint (§3.3).\\n3.1\\nModeling\\nColBERTv2 adopts the late interaction architecture\\nof ColBERT, depicted in Figure 1. Queries and pas-\\nsages are independently encoded with BERT (De-\\nvlin et al., 2019), and the output embeddings encod-\\ning each token are projected to a lower dimension.\\nDuring ofﬂine indexing, every passage d in the\\ncorpus is encoded into a set of vectors, and these\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 2\\nretrieval_score: 0.6972163446277386\\n\\na cross-encoder and hard-negative mining (§3.2)\\nto boost quality beyond any existing method, and\\nthen uses a residual compression mechanism (§3.3)\\nto reduce the space footprint of late interaction by\\n6–10× while preserving quality. As a result, Col-\\nBERTv2 establishes state-of-the-art retrieval qual-\\nity both within and outside its training domain with\\na competitive space footprint with typical single-\\nvector models.\\nWhen trained on MS MARCO Passage Rank-\\ning, ColBERTv2 achieves the highest MRR@10 of\\nany standalone retriever. In addition to in-domain\\nquality, we seek a retriever that generalizes “zero-\\nshot” to domain-speciﬁc corpora and long-tail top-\\nics, ones that are often under-represented in large\\npublic training sets. To this end, we evaluate Col-\\nBERTv2 on a wide array of out-of-domain bench-\\nmarks. These include three Wikipedia Open-QA\\nretrieval tests and 13 diverse retrieval and semantic-\\nsimilarity tasks from BEIR (Thakur et al., 2021). In\\naddition, we introduce a new benchmark, dubbed\\nLoTTE, for Long-Tail Topic-stratiﬁed Evaluation\\nfor IR that features 12 domain-speciﬁc search\\ntests, spanning StackExchange communities and\\nusing queries from GooAQ (Khashabi et al., 2021).\\nLoTTE focuses on relatively long-tail topics in\\nits passages, unlike the Open-QA tests and many\\nof the BEIR tasks, and evaluates models on their\\ncapacity to answer natural search queries with a\\npractical intent, unlike many of BEIR’s semantic-\\nsimilarity tasks. On 22 of 28 out-of-domain tests,\\nColBERTv2 achieves the highest quality, outper-\\nforming the next best retriever by up to 8% relative\\ngain, while using its compressed representations.\\nThis work makes the following contributions:\\n1. We propose ColBERTv2, a retriever that com-\\nbines denoised supervision and residual com-\\npression, leveraging the token-level decom-\\nposition of late interaction to achieve high\\nrobustness with a reduced space footprint.\\n2. We introduce LoTTE, a new resource for out-\\nof-domain evaluation of retrievers. LoTTE fo-\\ncuses on natural information-seeking queries\\nover long-tail topics, an important yet under-\\nstudied application space.\\n3. We evaluate ColBERTv2 across a wide range\\nof settings, establishing state-of-the-art qual-\\nity within and outside the training domain.\\n2\\nBackground & Related Work\\n2.1\\nToken-Decomposed Scoring in Neural IR\\nMany neural IR approaches encode passages as\\na single high-dimensional vector, trading off the\\nhigher quality of cross-encoders for improved ef-\\nﬁciency and scalability (Karpukhin et al., 2020;\\nXiong et al., 2020; Qu et al., 2021).\\nCol-\\nBERT’s (Khattab and Zaharia, 2020) late inter-\\naction paradigm addresses this tradeoff by com-\\nputing multi-vector embeddings and using a scal-\\nable “MaxSim” operator for retrieval.\\nSeveral\\nother systems leverage multi-vector representa-\\ntions, including Poly-encoders (Humeau et al.,\\n2020), PreTTR (MacAvaney et al., 2020), and\\nMORES (Gao et al., 2020), but these target\\nattention-based re-ranking as opposed to Col-\\nBERT’s scalable MaxSim end-to-end retrieval.\\nME-BERT (Luan et al., 2021) generates token-\\nlevel document embeddings similar to ColBERT,\\nbut retains a single embedding vector for queries.\\nCOIL (Gao et al., 2021) also generates token-level\\ndocument embeddings, but the token interactions\\nare restricted to lexical matching between query\\nand document terms. uniCOIL (Lin and Ma, 2021)\\nlimits the token embedding vectors of COIL to a\\nsingle dimension, reducing them to scalar weights\\nthat extend models like DeepCT (Dai and Callan,\\n2020) and DeepImpact (Mallia et al., 2021). To\\nproduce scalar weights, SPLADE (Formal et al.,\\n2021b) and SPLADEv2 (Formal et al., 2021a) pro-\\nduce a sparse vocabulary-level vector that retains\\nthe term-level decomposition of late interaction\\nwhile simplifying the storage into one dimension\\nper token. The SPLADE family also piggybacks on\\nthe language modeling capacity acquired by BERT\\nduring pretraining.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 17\\nretrieval_score: 0.7381827551664437\\n\\nSimilarly, on the HoVer (Jiang et al., 2020) dev\\nset, Baleen’s retrieval R@100 dropped from 92.2%\\nto only 90.6% but its sentence-level exact match\\nremained roughly the same, going from 39.2% to\\n39.4%. We hypothesize that the supervision meth-\\nods applied in ColBERTv2 (§3.2) can also be ap-\\nplied to lift quality in downstream tasks by improv-\\ning the recall of retrieval for these tasks. We leave\\nsuch exploration for future work.\\nC\\nRetrieval Latency\\nFigure 3 evaluates the latency of ColBERTv2\\nacross three collections of varying sizes, namely,\\nMS MARCO, LoTTE Pooled (dev), and LoTTE\\nLifestyle (dev), which contain approximately 9M\\npassages, 2.4M answer posts, and 270k answer\\nposts, respectively. We average latency across three\\nruns of the MS MARCO dev set and the LoTTE\\n“search” queries. Search is executed using a Titan\\nV GPU on a server with two Intel Xeon Gold 6132\\nCPUs, each with 28 hardware execution contexts.\\nThe ﬁgure varies three settings of ColBERTv2.\\nIn particular, we evaluate indexing with 1-bit and\\n2-bit encoding (§3.4) and searching by probing the\\nnearest 1, 2, or 4 centroids to each query vector\\n(§3.5). When probing probe centroids per vector,\\nwe score either probe × 212 or probe × 214 candi-\\ndates per query.8\\nTo begin with, we notice that the quality reported\\non the x-axis varies only within a relatively narrow\\nrange. For instance, the axis ranges from 38.50\\nthrough 39.75 for MS MARCO, and all but two of\\nthe cheapest settings score above 39.00. Similarly,\\nthe y-axis varies between approximately 50 mil-\\nliseconds per query up to 250 milliseconds (mostly\\nunder 150 milliseconds) using our relatively simple\\nPython-based implementation.\\nDigging deeper, we see that the best quality\\nin these metrics can be achieved or approached\\nclosely with around 100 milliseconds of latency\\nacross all three datasets, despite their various sizes\\nand characteristics, and that 2-bit indexing reliably\\noutperforms 1-bit indexing but the loss from more\\naggressive compression is small.\\nD\\nLoTTE\\nDomain coverage\\nTable 9 presents the full dis-\\ntribution of communities in the LoTTE dev dataset.\\n8These settings are selected based on preliminary explo-\\nration of these parameters, which indicated that performance\\nfor larger probe values tends to require scoring a larger num-\\nber of candidates.\\n0\\n200\\n400\\n600\\nWords per passage\\nPooled\\nLifestyle\\nTechnology\\nScience\\nRecreation\\nWriting\\nFigure 4: LoTTE words per passage\\n5\\n10\\n15\\n20\\nWords per query\\n[Forum] Pooled\\n[Forum] Lifestyle\\n[Forum] Technology\\n[Forum] Science\\n[Forum] Recreation\\n[Forum] Writing\\n[Search] Pooled\\n[Search] Lifestyle\\n[Search] Technology\\n[Search] Science\\n[Search] Recreation\\n[Search] Writing\\nFigure 5: LoTTE words per query\\nThe topics covered by LoTTE cover a wide range\\nof linguistic phenomena given the diversity in top-\\nics and communities represented. However, since\\nall posts are submitted by anonymous users we do\\nnot have demographic information regarding the\\nidentify of the contributors. All posts are written\\nin English.\\nPassages\\nAs mentioned in §4, we construct\\nLoTTE collections by selecting passages from the\\nStackExchange archive with positive scores. We\\nremove HTML tags from passages and ﬁlter out\\nempty passages. For each passage we record its\\ncorresponding query and save the query-to-passage\\nmapping to keep track of the posted answers corre-\\nsponding to each query.\\nSearch queries\\nWe construct the list of LoTTE\\nsearch queries by drawing from GooAQ queries\\nthat appear in the StackExchange post archive. We\\nﬁrst shufﬂe the list of GooAQ queries so that in\\ncases where multiple queries exist for the same\\nanswer passage we randomly select the query to\\ninclude in LoTTE rather than always selecting the\\nﬁrst appearing query. We verify that every query\\nhas at least one corresponding answer passage.\\n\\ntotal_pages: 20\\nfile_path: ColbertV2_2112.01488v3.pdf\\nsource: 7\\nretrieval_score: 0.7456166799640073\\n\\nQ: what is xerror in rpart?\\nQ: is sub question one word?\\nQ: how to open a garage door without making noise?\\nQ:\\nis docx and dotx the same?\\nQ: are upvotes and downvotes\\nanonymous?\\nQ: what is the difference between descriptive\\nessay and narrative essay?\\nQ: how to change default\\nuser proﬁle in chrome?\\nQ: does autohotkey need to be\\ninstalled?\\nQ: how do you tag someone on facebook with\\na youtube video?\\nQ: has mjolnir ever been broken?\\nQ: Snoopy can balance on an edge atop his doghouse. Is any\\nreason given for this?\\nQ: How many Ents were at the\\nEntmoot?\\nQ: What does a hexagonal sun tell us about\\nthe camera lens/sensor?\\nQ: Should I simply ignore it if\\nauthors assume that Im male in their response to my review of\\ntheir article?\\nQ: Why is the 2s orbital lower in energy than\\nthe 2p orbital when the electrons in 2s are usually farther from\\nthe nucleus?\\nQ: Are there reasons to use colour ﬁlters\\nwith digital cameras?\\nQ: How does the current know how\\nmuch to ﬂow, before having seen the resistor?\\nQ: What\\nis the difference between Fact and Truth?\\nQ: hAs a DM,\\nhow can I handle my Druid spying on everything with Wild\\nshape as a spider?\\nQ: What does 1x1 convolution mean\\nin a neural network?\\nTable 3: Comparison of a random sample of search\\nqueries (top) vs. forum queries (bottom).\\n2021b), we evaluate retrieval quality by comput-\\ning the success@5 (S@5) metric. Speciﬁcally, we\\naward a point to the system for each query where\\nit ﬁnds an accepted or upvoted (score ≥1) answer\\nfrom the target page in the top-5 hits.\\nAppendix D reports on the breakdown of con-\\nstituent communities per topic, the construction\\nprocedure of LoTTE as well as licensing considera-\\ntions, and relevant statistics. Figures 5 and 6 quan-\\ntitatively compare the search and forum queries.\\n5\\nEvaluation\\nWe now evaluate ColBERTv2 on passage retrieval\\ntasks, testing its quality within the training domain\\n(§5.1) as well as outside the training domain in\\nzero-shot settings (§5.2). Unless otherwise stated,\\nwe compress ColBERTv2 embeddings to b = 2\\nbits per dimension in our evaluation.\\n5.1\\nIn-Domain Retrieval Quality\\nSimilar to related work, we train for IR tasks on MS\\nMARCO Passage Ranking (Nguyen et al., 2016).\\nWithin the training domain, our development-set re-\\nsults are shown in Table 4, comparing ColBERTv2\\nwith vanilla ColBERT as well as state-of-the-art\\nsingle-vector systems.\\nWhile ColBERT outperforms single-vector sys-\\ntems like RepBERT, ANCE, and even TAS-B, im-\\nprovements in supervision such as distillation from\\ncross-encoders enable systems like SPLADEv2,\\nMethod\\nOfﬁcial Dev (7k)\\nLocal Eval (5k)\\nMRR@10 R@50 R@1k MRR@10 R@50 R@1k\\nModels without Distillation or Special Pretraining\\nRepBERT\\n30.4\\n-\\n94.3\\n-\\n-\\n-\\nDPR\\n31.1\\n-\\n95.2\\n-\\n-\\n-\\nANCE\\n33.0\\n-\\n95.9\\n-\\n-\\n-\\nLTRe\\n34.1\\n-\\n96.2\\n-\\n-\\n-\\nColBERT\\n36.0\\n82.9\\n96.8\\n36.7\\n-\\n-\\nModels with Distillation or Special Pretraining\\nTAS-B\\n34.7\\n-\\n97.8\\n-\\n-\\n-\\nSPLADEv2\\n36.8\\n-\\n97.9\\n37.9\\n84.9\\n98.0\\nPAIR\\n37.9\\n86.4\\n98.2\\n-\\n-\\n-\\ncoCondenser\\n38.2\\n-\\n98.4\\n-\\n-\\n-\\nRocketQAv2\\n38.8\\n86.2\\n98.1\\n39.8\\n85.8\\n97.9\\nColBERTv2\\n39.7\\n86.8\\n98.4\\n40.8\\n86.3\\n98.3\\nTable 4: In-domain performance on the development\\nset of MS MARCO Passage Ranking as well the “Local\\nEval” test set described by Khattab and Zaharia (2020).\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: How is ColbertV2 better than Colbert ?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "close.started\n",
      "close.started\n",
      "close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "close.complete\n",
      "close.complete\n",
      "close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2FCC8950>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2FCC8950>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2FCC8950>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2FCC8950>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020B3174C250> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020B3174C250> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020B3174C250> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020B3174C250> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2BD662A0>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2BD662A0>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2BD662A0>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020B2BD662A0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 26 Aug 2024 17:01:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'stuvalley'), (b'openai-processing-ms', b'1318'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3996179'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'57ms'), (b'x-request-id', b'req_6bb325761cfec7d13fc9328f46d405fc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8b9562f60a51914f-MAA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 26 Aug 2024 17:01:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'stuvalley'), (b'openai-processing-ms', b'1318'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3996179'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'57ms'), (b'x-request-id', b'req_6bb325761cfec7d13fc9328f46d405fc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8b9562f60a51914f-MAA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 26 Aug 2024 17:01:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'stuvalley'), (b'openai-processing-ms', b'1318'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3996179'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'57ms'), (b'x-request-id', b'req_6bb325761cfec7d13fc9328f46d405fc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8b9562f60a51914f-MAA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 26 Aug 2024 17:01:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'stuvalley'), (b'openai-processing-ms', b'1318'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15552000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'4000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'3996179'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'57ms'), (b'x-request-id', b'req_6bb325761cfec7d13fc9328f46d405fc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8b9562f60a51914f-MAA'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Mon, 26 Aug 2024 17:01:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'stuvalley', 'openai-processing-ms': '1318', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3996179', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '57ms', 'x-request-id': 'req_6bb325761cfec7d13fc9328f46d405fc', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8b9562f60a51914f-MAA', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Mon, 26 Aug 2024 17:01:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'stuvalley', 'openai-processing-ms': '1318', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3996179', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '57ms', 'x-request-id': 'req_6bb325761cfec7d13fc9328f46d405fc', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8b9562f60a51914f-MAA', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Mon, 26 Aug 2024 17:01:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'stuvalley', 'openai-processing-ms': '1318', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3996179', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '57ms', 'x-request-id': 'req_6bb325761cfec7d13fc9328f46d405fc', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8b9562f60a51914f-MAA', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Mon, 26 Aug 2024 17:01:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'stuvalley', 'openai-processing-ms': '1318', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '5000', 'x-ratelimit-limit-tokens': '4000000', 'x-ratelimit-remaining-requests': '4999', 'x-ratelimit-remaining-tokens': '3996179', 'x-ratelimit-reset-requests': '12ms', 'x-ratelimit-reset-tokens': '57ms', 'x-request-id': 'req_6bb325761cfec7d13fc9328f46d405fc', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8b9562f60a51914f-MAA', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_6bb325761cfec7d13fc9328f46d405fc\n",
      "request_id: req_6bb325761cfec7d13fc9328f46d405fc\n",
      "request_id: req_6bb325761cfec7d13fc9328f46d405fc\n",
      "request_id: req_6bb325761cfec7d13fc9328f46d405fc\n",
      "d8688276-4aaf-4204-9dbb-18ed13e3a5df\n",
      "Dev-set results for baseline systems are from their re-\n",
      "spective papers: Zhan et al. (2020b), Xiong et al. (2020)\n",
      "for DPR and ANCE, Zhan et al. (2020a), Khattab and\n",
      "Zaharia (2020), Hofstätter et al. (2021), Gao and Callan\n",
      "(2021), Ren et al. (2021a), Formal et al. (2021a), and\n",
      "Ren et al. (2021b).\n",
      "PAIR, and RocketQAv2 to achieve higher qual-\n",
      "ity than vanilla ColBERT. These supervision gains\n",
      "challenge the value of ﬁne-grained late interaction,\n",
      "and it is not inherently clear whether the stronger\n",
      "inductive biases of ColBERT-like models permit it\n",
      "to accept similar gains under distillation, especially\n",
      "when using compressed representations. Despite\n",
      "this, we ﬁnd that with denoised supervision and\n",
      "residual compression, ColBERTv2 achieves the\n",
      "highest quality across all systems. As we discuss\n",
      "in §5.3, it exhibits space footprint competitive with\n",
      "these single-vector models and much lower than\n",
      "vanilla ColBERT.\n",
      "Besides the ofﬁcial dev set, we evaluated Col-\n",
      "BERTv2, SPLADEv2, and RocketQAv2 on the\n",
      "“Local Eval” test set described by Khattab and Za-\n",
      "haria (2020) for MS MARCO, which consists of\n",
      "5000 queries disjoint with the training and the of-\n",
      "ﬁcial dev sets. These queries are obtained from\n",
      "labeled 50k queries that are provided in the ofﬁcial\n",
      "MS MARCO Passage Ranking task as additional\n",
      "validation data.4 On this test set, ColBERTv2 ob-\n",
      "tains 40.8% MRR@10, considerably outperform-\n",
      "ing the baselines, including RocketQAv2 which\n",
      "makes use of document titles in addition to the\n",
      "passage text unlike the other systems.\n",
      "4These are sampled from delta between qrels.dev.tsv\n",
      "and\n",
      "qrels.dev.small.tsv\n",
      "on\n",
      "https://microsoft.\n",
      "github.io/msmarco/Datasets. We refer to Khattab and\n",
      "Zaharia (2020) for details. All our query IDs will be made\n",
      "public to aid reproducibility.\n",
      "reranking score:  0.6589601635932922\n",
      "retrieval score:  0.705386269098783\n",
      "=====================================\n",
      "8f702d58-ad67-4684-a7f7-8f675103216e\n",
      "In\n",
      "practice, user-facing IR and QA applications often\n",
      "pertain to domain-speciﬁc corpora, for which little\n",
      "to no training data is available and whose topics\n",
      "are under-represented in large public collections.\n",
      "This out-of-domain regime has received recent\n",
      "attention with the BEIR (Thakur et al., 2021) bench-\n",
      "mark. BEIR combines several existing datasets\n",
      "into a heterogeneous suite for “zero-shot IR” tasks,\n",
      "spanning bio-medical, ﬁnancial, and scientiﬁc do-\n",
      "mains. While the BEIR datasets provide a use-\n",
      "ful testbed, many capture broad semantic related-\n",
      "ness tasks—like citations, counter arguments, or\n",
      "duplicate questions–instead of natural search tasks,\n",
      "or else they focus on high-popularity entities like\n",
      "those in Wikipedia. In §4, we introduce LoTTE, a\n",
      "new dataset for out-of-domain retrieval, exhibiting\n",
      "natural search queries over long-tail topics.\n",
      "3\n",
      "ColBERTv2\n",
      "We now introduce ColBERTv2, which improves\n",
      "the quality of multi-vector retrieval models (§3.2)\n",
      "while reducing their space footprint (§3.3).\n",
      "3.1\n",
      "Modeling\n",
      "ColBERTv2 adopts the late interaction architecture\n",
      "of ColBERT, depicted in Figure 1. Queries and pas-\n",
      "sages are independently encoded with BERT (De-\n",
      "vlin et al., 2019), and the output embeddings encod-\n",
      "ing each token are projected to a lower dimension.\n",
      "During ofﬂine indexing, every passage d in the\n",
      "corpus is encoded into a set of vectors, and these\n",
      "reranking score:  0.6474084854125977\n",
      "retrieval score:  0.6939909166632486\n",
      "=====================================\n",
      "7192a93b-5adf-482b-a495-899e2db20c2f\n",
      "a cross-encoder and hard-negative mining (§3.2)\n",
      "to boost quality beyond any existing method, and\n",
      "then uses a residual compression mechanism (§3.3)\n",
      "to reduce the space footprint of late interaction by\n",
      "6–10× while preserving quality. As a result, Col-\n",
      "BERTv2 establishes state-of-the-art retrieval qual-\n",
      "ity both within and outside its training domain with\n",
      "a competitive space footprint with typical single-\n",
      "vector models.\n",
      "When trained on MS MARCO Passage Rank-\n",
      "ing, ColBERTv2 achieves the highest MRR@10 of\n",
      "any standalone retriever. In addition to in-domain\n",
      "quality, we seek a retriever that generalizes “zero-\n",
      "shot” to domain-speciﬁc corpora and long-tail top-\n",
      "ics, ones that are often under-represented in large\n",
      "public training sets. To this end, we evaluate Col-\n",
      "BERTv2 on a wide array of out-of-domain bench-\n",
      "marks. These include three Wikipedia Open-QA\n",
      "retrieval tests and 13 diverse retrieval and semantic-\n",
      "similarity tasks from BEIR (Thakur et al., 2021). In\n",
      "addition, we introduce a new benchmark, dubbed\n",
      "LoTTE, for Long-Tail Topic-stratiﬁed Evaluation\n",
      "for IR that features 12 domain-speciﬁc search\n",
      "tests, spanning StackExchange communities and\n",
      "using queries from GooAQ (Khashabi et al., 2021).\n",
      "LoTTE focuses on relatively long-tail topics in\n",
      "its passages, unlike the Open-QA tests and many\n",
      "of the BEIR tasks, and evaluates models on their\n",
      "capacity to answer natural search queries with a\n",
      "practical intent, unlike many of BEIR’s semantic-\n",
      "similarity tasks. On 22 of 28 out-of-domain tests,\n",
      "ColBERTv2 achieves the highest quality, outper-\n",
      "forming the next best retriever by up to 8% relative\n",
      "gain, while using its compressed representations.\n",
      "This work makes the following contributions:\n",
      "1. We propose ColBERTv2, a retriever that com-\n",
      "bines denoised supervision and residual com-\n",
      "pression, leveraging the token-level decom-\n",
      "position of late interaction to achieve high\n",
      "robustness with a reduced space footprint.\n",
      "2. We introduce LoTTE, a new resource for out-\n",
      "of-domain evaluation of retrievers. LoTTE fo-\n",
      "cuses on natural information-seeking queries\n",
      "over long-tail topics, an important yet under-\n",
      "studied application space.\n",
      "3. We evaluate ColBERTv2 across a wide range\n",
      "of settings, establishing state-of-the-art qual-\n",
      "ity within and outside the training domain.\n",
      "2\n",
      "Background & Related Work\n",
      "2.1\n",
      "Token-Decomposed Scoring in Neural IR\n",
      "Many neural IR approaches encode passages as\n",
      "a single high-dimensional vector, trading off the\n",
      "higher quality of cross-encoders for improved ef-\n",
      "ﬁciency and scalability (Karpukhin et al., 2020;\n",
      "Xiong et al., 2020; Qu et al., 2021).\n",
      "Col-\n",
      "BERT’s (Khattab and Zaharia, 2020) late inter-\n",
      "action paradigm addresses this tradeoff by com-\n",
      "puting multi-vector embeddings and using a scal-\n",
      "able “MaxSim” operator for retrieval.\n",
      "Several\n",
      "other systems leverage multi-vector representa-\n",
      "tions, including Poly-encoders (Humeau et al.,\n",
      "2020), PreTTR (MacAvaney et al., 2020), and\n",
      "MORES (Gao et al., 2020), but these target\n",
      "attention-based re-ranking as opposed to Col-\n",
      "BERT’s scalable MaxSim end-to-end retrieval.\n",
      "ME-BERT (Luan et al., 2021) generates token-\n",
      "level document embeddings similar to ColBERT,\n",
      "but retains a single embedding vector for queries.\n",
      "COIL (Gao et al., 2021) also generates token-level\n",
      "document embeddings, but the token interactions\n",
      "are restricted to lexical matching between query\n",
      "and document terms. uniCOIL (Lin and Ma, 2021)\n",
      "limits the token embedding vectors of COIL to a\n",
      "single dimension, reducing them to scalar weights\n",
      "that extend models like DeepCT (Dai and Callan,\n",
      "2020) and DeepImpact (Mallia et al., 2021). To\n",
      "produce scalar weights, SPLADE (Formal et al.,\n",
      "2021b) and SPLADEv2 (Formal et al., 2021a) pro-\n",
      "duce a sparse vocabulary-level vector that retains\n",
      "the term-level decomposition of late interaction\n",
      "while simplifying the storage into one dimension\n",
      "per token. The SPLADE family also piggybacks on\n",
      "the language modeling capacity acquired by BERT\n",
      "during pretraining.\n",
      "reranking score:  0.6300389766693115\n",
      "retrieval score:  0.6972163446277386\n",
      "=====================================\n",
      "7285a6da-978c-4ef2-a006-7ff228f57c53\n",
      "Similarly, on the HoVer (Jiang et al., 2020) dev\n",
      "set, Baleen’s retrieval R@100 dropped from 92.2%\n",
      "to only 90.6% but its sentence-level exact match\n",
      "remained roughly the same, going from 39.2% to\n",
      "39.4%. We hypothesize that the supervision meth-\n",
      "ods applied in ColBERTv2 (§3.2) can also be ap-\n",
      "plied to lift quality in downstream tasks by improv-\n",
      "ing the recall of retrieval for these tasks. We leave\n",
      "such exploration for future work.\n",
      "C\n",
      "Retrieval Latency\n",
      "Figure 3 evaluates the latency of ColBERTv2\n",
      "across three collections of varying sizes, namely,\n",
      "MS MARCO, LoTTE Pooled (dev), and LoTTE\n",
      "Lifestyle (dev), which contain approximately 9M\n",
      "passages, 2.4M answer posts, and 270k answer\n",
      "posts, respectively. We average latency across three\n",
      "runs of the MS MARCO dev set and the LoTTE\n",
      "“search” queries. Search is executed using a Titan\n",
      "V GPU on a server with two Intel Xeon Gold 6132\n",
      "CPUs, each with 28 hardware execution contexts.\n",
      "The ﬁgure varies three settings of ColBERTv2.\n",
      "In particular, we evaluate indexing with 1-bit and\n",
      "2-bit encoding (§3.4) and searching by probing the\n",
      "nearest 1, 2, or 4 centroids to each query vector\n",
      "(§3.5). When probing probe centroids per vector,\n",
      "we score either probe × 212 or probe × 214 candi-\n",
      "dates per query.8\n",
      "To begin with, we notice that the quality reported\n",
      "on the x-axis varies only within a relatively narrow\n",
      "range. For instance, the axis ranges from 38.50\n",
      "through 39.75 for MS MARCO, and all but two of\n",
      "the cheapest settings score above 39.00. Similarly,\n",
      "the y-axis varies between approximately 50 mil-\n",
      "liseconds per query up to 250 milliseconds (mostly\n",
      "under 150 milliseconds) using our relatively simple\n",
      "Python-based implementation.\n",
      "Digging deeper, we see that the best quality\n",
      "in these metrics can be achieved or approached\n",
      "closely with around 100 milliseconds of latency\n",
      "across all three datasets, despite their various sizes\n",
      "and characteristics, and that 2-bit indexing reliably\n",
      "outperforms 1-bit indexing but the loss from more\n",
      "aggressive compression is small.\n",
      "D\n",
      "LoTTE\n",
      "Domain coverage\n",
      "Table 9 presents the full dis-\n",
      "tribution of communities in the LoTTE dev dataset.\n",
      "8These settings are selected based on preliminary explo-\n",
      "ration of these parameters, which indicated that performance\n",
      "for larger probe values tends to require scoring a larger num-\n",
      "ber of candidates.\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "Words per passage\n",
      "Pooled\n",
      "Lifestyle\n",
      "Technology\n",
      "Science\n",
      "Recreation\n",
      "Writing\n",
      "Figure 4: LoTTE words per passage\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "Words per query\n",
      "[Forum] Pooled\n",
      "[Forum] Lifestyle\n",
      "[Forum] Technology\n",
      "[Forum] Science\n",
      "[Forum] Recreation\n",
      "[Forum] Writing\n",
      "[Search] Pooled\n",
      "[Search] Lifestyle\n",
      "[Search] Technology\n",
      "[Search] Science\n",
      "[Search] Recreation\n",
      "[Search] Writing\n",
      "Figure 5: LoTTE words per query\n",
      "The topics covered by LoTTE cover a wide range\n",
      "of linguistic phenomena given the diversity in top-\n",
      "ics and communities represented. However, since\n",
      "all posts are submitted by anonymous users we do\n",
      "not have demographic information regarding the\n",
      "identify of the contributors. All posts are written\n",
      "in English.\n",
      "Passages\n",
      "As mentioned in §4, we construct\n",
      "LoTTE collections by selecting passages from the\n",
      "StackExchange archive with positive scores. We\n",
      "remove HTML tags from passages and ﬁlter out\n",
      "empty passages. For each passage we record its\n",
      "corresponding query and save the query-to-passage\n",
      "mapping to keep track of the posted answers corre-\n",
      "sponding to each query.\n",
      "Search queries\n",
      "We construct the list of LoTTE\n",
      "search queries by drawing from GooAQ queries\n",
      "that appear in the StackExchange post archive. We\n",
      "ﬁrst shufﬂe the list of GooAQ queries so that in\n",
      "cases where multiple queries exist for the same\n",
      "answer passage we randomly select the query to\n",
      "include in LoTTE rather than always selecting the\n",
      "ﬁrst appearing query. We verify that every query\n",
      "has at least one corresponding answer passage.\n",
      "reranking score:  0.6248947381973267\n",
      "retrieval score:  0.7381827551664437\n",
      "=====================================\n",
      "667e6482-bade-419a-b454-f250400357ec\n",
      "Q: what is xerror in rpart?\n",
      "Q: is sub question one word?\n",
      "Q: how to open a garage door without making noise?\n",
      "Q:\n",
      "is docx and dotx the same?\n",
      "Q: are upvotes and downvotes\n",
      "anonymous?\n",
      "Q: what is the difference between descriptive\n",
      "essay and narrative essay?\n",
      "Q: how to change default\n",
      "user proﬁle in chrome?\n",
      "Q: does autohotkey need to be\n",
      "installed?\n",
      "Q: how do you tag someone on facebook with\n",
      "a youtube video?\n",
      "Q: has mjolnir ever been broken?\n",
      "Q: Snoopy can balance on an edge atop his doghouse. Is any\n",
      "reason given for this?\n",
      "Q: How many Ents were at the\n",
      "Entmoot?\n",
      "Q: What does a hexagonal sun tell us about\n",
      "the camera lens/sensor?\n",
      "Q: Should I simply ignore it if\n",
      "authors assume that Im male in their response to my review of\n",
      "their article?\n",
      "Q: Why is the 2s orbital lower in energy than\n",
      "the 2p orbital when the electrons in 2s are usually farther from\n",
      "the nucleus?\n",
      "Q: Are there reasons to use colour ﬁlters\n",
      "with digital cameras?\n",
      "Q: How does the current know how\n",
      "much to ﬂow, before having seen the resistor?\n",
      "Q: What\n",
      "is the difference between Fact and Truth?\n",
      "Q: hAs a DM,\n",
      "how can I handle my Druid spying on everything with Wild\n",
      "shape as a spider?\n",
      "Q: What does 1x1 convolution mean\n",
      "in a neural network?\n",
      "Table 3: Comparison of a random sample of search\n",
      "queries (top) vs. forum queries (bottom).\n",
      "2021b), we evaluate retrieval quality by comput-\n",
      "ing the success@5 (S@5) metric. Speciﬁcally, we\n",
      "award a point to the system for each query where\n",
      "it ﬁnds an accepted or upvoted (score ≥1) answer\n",
      "from the target page in the top-5 hits.\n",
      "Appendix D reports on the breakdown of con-\n",
      "stituent communities per topic, the construction\n",
      "procedure of LoTTE as well as licensing considera-\n",
      "tions, and relevant statistics. Figures 5 and 6 quan-\n",
      "titatively compare the search and forum queries.\n",
      "5\n",
      "Evaluation\n",
      "We now evaluate ColBERTv2 on passage retrieval\n",
      "tasks, testing its quality within the training domain\n",
      "(§5.1) as well as outside the training domain in\n",
      "zero-shot settings (§5.2). Unless otherwise stated,\n",
      "we compress ColBERTv2 embeddings to b = 2\n",
      "bits per dimension in our evaluation.\n",
      "5.1\n",
      "In-Domain Retrieval Quality\n",
      "Similar to related work, we train for IR tasks on MS\n",
      "MARCO Passage Ranking (Nguyen et al., 2016).\n",
      "Within the training domain, our development-set re-\n",
      "sults are shown in Table 4, comparing ColBERTv2\n",
      "with vanilla ColBERT as well as state-of-the-art\n",
      "single-vector systems.\n",
      "While ColBERT outperforms single-vector sys-\n",
      "tems like RepBERT, ANCE, and even TAS-B, im-\n",
      "provements in supervision such as distillation from\n",
      "cross-encoders enable systems like SPLADEv2,\n",
      "Method\n",
      "Ofﬁcial Dev (7k)\n",
      "Local Eval (5k)\n",
      "MRR@10 R@50 R@1k MRR@10 R@50 R@1k\n",
      "Models without Distillation or Special Pretraining\n",
      "RepBERT\n",
      "30.4\n",
      "-\n",
      "94.3\n",
      "-\n",
      "-\n",
      "-\n",
      "DPR\n",
      "31.1\n",
      "-\n",
      "95.2\n",
      "-\n",
      "-\n",
      "-\n",
      "ANCE\n",
      "33.0\n",
      "-\n",
      "95.9\n",
      "-\n",
      "-\n",
      "-\n",
      "LTRe\n",
      "34.1\n",
      "-\n",
      "96.2\n",
      "-\n",
      "-\n",
      "-\n",
      "ColBERT\n",
      "36.0\n",
      "82.9\n",
      "96.8\n",
      "36.7\n",
      "-\n",
      "-\n",
      "Models with Distillation or Special Pretraining\n",
      "TAS-B\n",
      "34.7\n",
      "-\n",
      "97.8\n",
      "-\n",
      "-\n",
      "-\n",
      "SPLADEv2\n",
      "36.8\n",
      "-\n",
      "97.9\n",
      "37.9\n",
      "84.9\n",
      "98.0\n",
      "PAIR\n",
      "37.9\n",
      "86.4\n",
      "98.2\n",
      "-\n",
      "-\n",
      "-\n",
      "coCondenser\n",
      "38.2\n",
      "-\n",
      "98.4\n",
      "-\n",
      "-\n",
      "-\n",
      "RocketQAv2\n",
      "38.8\n",
      "86.2\n",
      "98.1\n",
      "39.8\n",
      "85.8\n",
      "97.9\n",
      "ColBERTv2\n",
      "39.7\n",
      "86.8\n",
      "98.4\n",
      "40.8\n",
      "86.3\n",
      "98.3\n",
      "Table 4: In-domain performance on the development\n",
      "set of MS MARCO Passage Ranking as well the “Local\n",
      "Eval” test set described by Khattab and Zaharia (2020).\n",
      "reranking score:  0.6163658499717712\n",
      "retrieval score:  0.7456166799640073\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How is ColbertV2 better than Colbert ?\",\n",
    ")\n",
    "\n",
    "# Print the reranked results\n",
    "for node in response.source_nodes:\n",
    "    print(node.id_)\n",
    "    print(node.node.get_content())\n",
    "    print(\"reranking score: \", node.score)\n",
    "    print(\"retrieval score: \", node.node.metadata.get(\"retrieval_score\", \"N/A\"))\n",
    "    print(\"=====================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f77a770a-88e5-46da-861c-61b4ce85480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColbertV2 achieves higher quality than vanilla Colbert by combining denoised supervision and residual compression, leading to improved robustness with a reduced space footprint. Additionally, ColbertV2 exhibits state-of-the-art retrieval quality both within and outside its training domain, outperforming other retrievers on various out-of-domain benchmarks.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f5bca-b163-4928-999d-12b98af04809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3193e9-38a1-4b67-8f8b-12faba43cdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00633c2-a83d-4f8e-afdc-71eb0815acd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
